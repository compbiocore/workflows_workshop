{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4601bc",
   "metadata": {},
   "source": [
    "<h1><center>Using Nextflow on OSCAR: Basic Bioinformatics Workflows</center></h1>\n",
    "<p><center>Instructors: Jordan Lawson and Paul Cao</center>\n",
    " <center>Center for Computation and Visualization</center>\n",
    " <center>Center for Computational Biology of Human Disease - Computational Biology Core</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfe5ec",
   "metadata": {},
   "source": [
    "Resources for help @brown <br> \n",
    "\n",
    "COBRE CBHD Computational Biology Core\n",
    "- Office hours\n",
    "- https://cbc.brown.edu\n",
    "- slack channel on ccv-share\n",
    "- cbc-help@brown.edu <br>\n",
    "\n",
    "Center for Computation and Visualization\n",
    "- Office hours\n",
    "- https://ccv.brown.edu\n",
    "- ccv-share slack channel\n",
    "- https://docs.ccv.brown.edu\n",
    "- support@ccv.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2fc35",
   "metadata": {},
   "source": [
    "## What is Covered in This Tutorial?  \n",
    "\n",
    "In a previous workshop, we went over simple \"Hello World\" and a toy workflow. In this workshop, we will go over: \n",
    "\n",
    "- How to understand and run a `nf-core` pre-built workflow on OSCAR\n",
    "- How to build or extend a Docker container and run it in a Nextflow pipeline\n",
    "- More advanced Nextflow pipelining tactics such as:\n",
    "    - Conditionals and optional parameters\n",
    "    - Parsing a samplesheet of a large experiment\n",
    "    - Asynchronous and synchronous job execution\n",
    "    - Putting it all together in an simple RNASeq example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372c107",
   "metadata": {},
   "source": [
    "<img src=\"./img/nextflow.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872e2d",
   "metadata": {},
   "source": [
    "### Step 1: The Setup (for folks who haven't set up their `nextflow_start` script previously):\n",
    "\n",
    "Let's first discuss setting up our environment on OSCAR so that we can get Nextflow up and running. \n",
    "\n",
    "**At this point, I am going to open my terminal on Open OnDemand (OOD) so that I can walk you through and show you how each of these steps and files below look. Feel free to open your terminal as well and follow along. To do so, you can go to OOD at https://ood.ccv.brown.edu and under the Clusters tab at the top select the >_OSCAR Shell Access option. All files used today can be found on GitHub in the folder at: https://github.com/compbiocore/workflows_on_OSCAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba3ec3",
   "metadata": {},
   "source": [
    "#### Step 1a: In OOD terminal, set up Nextflow Configuration Script using `compbiocore/workflows_on_OSCAR`:\n",
    "\n",
    "```bash\n",
    "[pcao5@node1322 ~]$ cd ~/\n",
    "[pcao5@node1322 ~]$ git clone https://github.com/compbiocore/workflows_on_OSCAR.git\n",
    "[pcao5@node1322 ~]$ git clone https://github.com/compbiocore/workflows_workshop.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288e2bb",
   "metadata": {},
   "source": [
    "#### Step 1b: Install `compbiocore/workflows_on_OSCAR` package:\n",
    "\n",
    "```bash\n",
    "bash ~/workflows_on_OSCAR/install_me/install.sh && source ~/.bashrc\n",
    "```\n",
    "\n",
    "\n",
    "For the 1st installation prompt, input `NextFlow`:\n",
    "\n",
    "```bash\n",
    "Welcome to a program designed to help you easily set up and run workflow management systems on OSCAR!\n",
    "\n",
    "Please type which software you wish to use: Nextflow or Snakemake? Nextflow\n",
    "```\n",
    "\n",
    "For the 2nd installation prompt, input your GitHub username (e.g., `paulcao-brown`):\n",
    "\n",
    "```bash\n",
    "Nextflow software detected, initializing configuration...\n",
    "What is your GitHub user name? paulcao-brown\n",
    "What is your GitHub token (we will keep this secret) - [Hit Enter when Done]?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd311f",
   "metadata": {},
   "source": [
    "#### Step 1c: Create a new GitHub Token and enter it:\n",
    "<img src=\"https://i.imgur.com/GBGDQhY.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6a68b",
   "metadata": {},
   "source": [
    "#### Step 1d: Complete the Installation \n",
    "\n",
    "\n",
    "```bash\n",
    "Currently the Nextflow default for HPC resources is: memory = 5.GB time = 2.h cpus = 2 \n",
    "Do you want to change these default resources for your Nextflow pipeline [Yes or No]? No\n",
    "Keeping defaults!\n",
    "\n",
    "OUTPUT MESSAGE:\n",
    "\n",
    "                ******************************************************************\n",
    "                 NEXTFLOW is now set up and configured and ready to run on OSCAR!\n",
    "                ******************************************************************\n",
    "                \n",
    "\n",
    "Your default resources for Nextflow are: memory = 5.GB time = 2.h cpus = 2 \n",
    "\n",
    "\n",
    "                To further customize your pipeline for efficiency, you can enter the following \n",
    "                label '<LabelName>' options right within processes in your Nextflow .nf script:\n",
    "                1. label 'OSCAR_small_job' (comes with memory = 4 GB, time = 1 hour, cpus = 1)\n",
    "                2. label 'OSCAR_medium_job' (comes with memory = 8 GB, time = 16 hours, cpus = 2)\n",
    "                3. label 'OSCAR_large_job' (comes with memory = 16 GB, time = 24 hours, cpus = 2)\n",
    "                \n",
    "\n",
    "README:\n",
    "\n",
    "Please see https://github.com/compbiocore/workflows_on_OSCAR for further details on how to add the above label options to your workflow.\n",
    "\n",
    "Note the setup is designed such that pipelines downloaded from nf-core with their own resource specs within the .nf script will override your defaults.\n",
    "\n",
    "To run Nextflow commands, you must first type and run the nextflow_start command.\n",
    "\n",
    "To further learn how to easily run your Nextflow pipelines on OSCAR, use the Nextflow template shell script located in your ~/nextflow_setup directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4c1e",
   "metadata": {},
   "source": [
    "### Step 2: Running a pre-existing `nf-core` Pipeline\n",
    "\n",
    "In the previous Nextflow workshop, we built a toy pipeline from scratch. However, sometimes we want to run more complicated workflows where we adapt/copy an existing Bioinformatics pipeline that the `nf-core` community has open-sourced instead of re-inventing the wheel. \n",
    "\n",
    "For this exercise, we are once again using the RRBS example that we started with, so we need a pipeline that is appropriate to use for processing RRBS data. Heading over to https://nf-co.re/pipelines we can see that the **methylseq** pipeline will work for our data. We can view the details of this pipeline, such as all of its arguments and the steps it performs, by cliking on its link or visiting here: https://nf-co.re/methylseq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabbaa1",
   "metadata": {},
   "source": [
    "#### Step 2a. Running methylSeq\n",
    "\n",
    "Virtually every one of `nf-core`'s workflows have a test functionality that allow us to explore how the protocol works without the hassle of setting up our own test samplesheets/inputs. \n",
    "\n",
    "\n",
    "#####  Running methylSeq test-case using `--profile test, singularity`\n",
    "\n",
    "```bash\n",
    "nextflow run nf-core/methylseq -profile test,singularity --outdir methylseq_out\n",
    "```\n",
    "\n",
    "#####  Example output:\n",
    "\n",
    "![](https://i.imgur.com/UGXkZAZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba68b2",
   "metadata": {},
   "source": [
    "#### Step 2b. Inspecting the Test Workflow Results:\n",
    "\n",
    "Once a test workflow has finished running, the `nf-core` pipeline willproduce a folder labeled `$OUT_DIR/pipeline_info`. In here, you will find: \n",
    "- Execution report \n",
    "    - The flow of every step of the test pipeline that was run\n",
    "    - The exact commands of each step\n",
    "- samplesheet.valid.csv (the test sample sheet)\n",
    "- software_version.yml (the exact versions of the software used)\n",
    "\n",
    "##### Example of a `pipeline_info` directory on nf-core: https://nf-co.re/methylseq/results#methylseq/results-93bc5811603c287c766a0ff7e03b5b41f4483895/bismark/pipeline_info/pipeline_dag_2022-12-17_00-46-10.html\n",
    "\n",
    "##### Trace Log: \n",
    "![](https://i.imgur.com/YsjYMNe.png)\n",
    "\n",
    "##### Audit Log of Every Step (with Command):\n",
    "![](https://i.imgur.com/qdJw4qT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa47221",
   "metadata": {},
   "source": [
    "#### Step 2b. Modeling this Test Run and Extracting its Test Inputs and Samplesheet: \n",
    "\n",
    "Although we have run this test workflow and can audit the steps, we still would like to know what were the parameters that were used to run this test case. We can do so by looking at the `conf/test.config` of every `nf-core` workflow: \n",
    "\n",
    "##### Inspect the `conf/test.config`:\n",
    "\n",
    "https://github.com/nf-core/methylseq/blob/master/conf/test.config: \n",
    "```bash\n",
    "    // Input data\n",
    "    input = \"$test_data_base/samplesheet/samplesheet_test.csv\"\n",
    "\n",
    "    // Genome references\n",
    "    fasta = \"$test_data_base/reference/genome.fa\"\n",
    "    fasta_index = \"$test_data_base/reference/genome.fa.fai\"\n",
    "```\n",
    "\n",
    "##### Replicate the same run (but with our own local files and specifying the exact parameters):\n",
    "\n",
    "From here, we can `wget` all of test input files and understand that the necessary parameters that were run `--fasta`, `--fasta_index` and `--input`.\n",
    "\n",
    "```bash\n",
    "#curl these files so we can use the command and original inputs locally as templates to run our workflow\n",
    "wget https://github.com/nf-core/test-datasets/tree/methylseq/samplesheet/samplesheet_test.csv\n",
    "wget https://github.com/nf-core/test-datasets/tree/methylseq/reference/genome.fa\n",
    "wget https://github.com/nf-core/test-datasets/tree/methylseq/reference/genome.fa.fai\n",
    "\n",
    "nextflow run nf-core/methylseq --input samplesheet_test.csv --fasta genome.fa --fasta_index genome.fa.fai --outdir methylseq_out2\n",
    "```\n",
    "\n",
    "##### View the Samplesheet:\n",
    "\n",
    "From this samplesheet, we can now understand exactly how to structure our real potential reads for our next real run.\n",
    "\n",
    "https://github.com/nf-core/test-datasets/tree/methylseq/samplesheet/samplesheet_test.csv:\n",
    "![](https://i.imgur.com/JHFvh3B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3a6b8",
   "metadata": {},
   "source": [
    "##### Step 2c. Read the `nf-core` Pipeline Documentation\n",
    "\n",
    "We have audited the test case pipeline, its inputs and samplesheets. However, there still might be additional test-cases and parameters we would like to know. We can find all of them out on the well-documented page of each `nf-core`'s pipeline. \n",
    "\n",
    "Drawing on the current `methylseq` example, let's take a look here: https://nf-co.re/methylseq/2.3.0/parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a6e28",
   "metadata": {},
   "source": [
    "### Step 3. Building or Adapting Your Own Docker Container\n",
    "\n",
    "At times it may be necessary to either build your own Docker containers or adapt existing Docker/Singularity containers when running your workflow. The following sections will demonstrate how to build or adapt existing containers into your Nextflow workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf20e4",
   "metadata": {},
   "source": [
    "#### Build a Docker Container from scratch\n",
    "\n",
    "Suppose you might like to use an alternative reads aligner, `salmon`; which is not available on OSCAR and so you would like to build a docker container so you can run it on OSCAR. The steps to do so would be as follows: \n",
    "\n",
    "###### 3a. First make a `Dockerfile`:\n",
    "```bash\n",
    "FROM ubuntu:lunar\n",
    "\n",
    "RUN apt-get update && apt-get install -y curl cowsay\n",
    "RUN curl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz | tar xz \\\n",
    "&& mv /salmon-*/bin/* /usr/bin/ \\\n",
    "&& mv /salmon-*/lib/* /usr/lib/\n",
    "```\n",
    "\n",
    "###### 3b. Build the Docker image (targetting `linux/amd64` for OSCAR and tagged as `salmon:latest`):\n",
    "Explicitly specifying the Docker build platform of `linux/amd64` is necessary to make your container compatible with OSCAR (a Scientific Linux environment) versus your local environment which may or may not be compatible (e.g., MacOS ARM64 or Windows PC)./\n",
    "\n",
    "```bash\n",
    "docker build -f salmon_Dockerfile -t salmon:latest --platform linux/amd64 .\n",
    "```\n",
    "\n",
    "###### 3c. Upload to your DockerHub account:\n",
    "Uploading your Docker container to the cloud then enables Nextflow running on OSCAR to pull down your container from `Dockerhub` or other container registries and use them. \n",
    "\n",
    "```bash\n",
    "docker tag salmon:latest $DOCKER_USER/salmon:latest\n",
    "docker push $DOCKER_USER/salmon:latest\n",
    "```\n",
    "\n",
    "###### Example Output:\n",
    "![](https://i.imgur.com/OYPbYSs.png)\n",
    "\n",
    "###### 3d. Reference your container with its `DockerHub` id and tag in workflow\n",
    "This will instruct the Nextflow pipeline to automatically pull down your Docker container and convert them into a Singularity image (which is a safer/more compatible container that `OSCAR` prefers to run):\n",
    "\n",
    "\n",
    "###### salmon.nf:\n",
    "```bash\n",
    "process salmon {\n",
    "    container 'cowmoo/salmon:latest'\n",
    "\n",
    "    output:\n",
    "     stdout\n",
    "\n",
    "    script:\n",
    "    \"\"\"\n",
    "    salmon -h\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  salmon() | view\n",
    "}\n",
    "```\n",
    "\n",
    "###### run salmon.nf:\n",
    "\n",
    "```bash\n",
    "nextflow run salmon.nf\n",
    "```\n",
    "\n",
    "###### Example output that confirms Salmon container now works on `OSCAR`:\n",
    "![](https://i.imgur.com/Szv8yq5.png)\n",
    "\n",
    "\n",
    "###### 3e. Leverage and Use Pre-Built Docker Containers in `nf-core`:\n",
    "\n",
    "\n",
    "We don't have to always build containers from scratch. We can also refer to and draw on the rich repository of `nf-core` or `biocontainers`. For example, we can find the Singualarity image url for every process defined in a `nf-core` workflow (they are always stored in `modules/nf-core/$PROCESS`). We will use `bismark` in `nf-core/methylseq` as an example:\n",
    "\n",
    "##### Find the `bismark` process definition: \n",
    "\n",
    "\n",
    "###### https://github.com/nf-core/methylseq/blob/master/modules/nf-core/bismark/align/main.nf:\n",
    "```bash\n",
    "process BISMARK_ALIGN {\n",
    "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
    "        'https://depot.galaxyproject.org/singularity/bismark:0.24.0--hdfd78af_0' :\n",
    "        'quay.io/biocontainers/bismark:0.24.0--hdfd78af_0' }\"\n",
    "    \n",
    "    \"\"\"\n",
    "    bismark \\\\\n",
    "        $fastq \\\\\n",
    "        --genome $index \\\\\n",
    "        --bam \\\\\n",
    "        $args\n",
    "    cat <<-END_VERSIONS > versions.yml\n",
    "    \"${task.process}\":\n",
    "        bismark: \\$(echo \\$(bismark -v 2>&1) | sed 's/^.*Bismark Version: v//; s/Copyright.*\\$//')\n",
    "    END_VERSIONS\n",
    "    \"\"\"\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "##### Copy it into our workflow:\n",
    "\n",
    "We can simply grab the singularity container url: `'https://depot.galaxyproject.org/singularity/bismark:0.24.0--hdfd78af_0'`\n",
    "\n",
    "```bash\n",
    "process BISMARK_ALIGN {\n",
    "    container 'https://depot.galaxyproject.org/singularity/bismark:0.24.0--hdfd78af_0'\n",
    "\n",
    "    output:\n",
    "     stdout\n",
    "\n",
    "    script:\n",
    "    \"\"\"\n",
    "    bismark\n",
    "    \"\"\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d186e48",
   "metadata": {},
   "source": [
    "### Step 4. Advanced Nextflow Patterns:\n",
    "\n",
    "In the previous section, we examined a basic Nextflow workflow and the tools you use with it, such as containers. However, this example was quite rudimentary; for more complex workflows we may not be able to get by with just simply chaining together simple tasks. For example, to utilize more complicated workflows, we may have to leverage: \n",
    "\n",
    "- Conditionals and optional parameters\n",
    "- Parsing a samplesheet of a large experiment\n",
    "- Asynchronous and synchronous job execution (or scatter-gather pattern)\n",
    "\n",
    "We will learn all of these patterns with some toy examples and then apply and put all of the concepts together in a small RNASeq example workflow demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c13fff",
   "metadata": {},
   "source": [
    "#### Step 4a. Refresher using the Word Count Example\n",
    "\n",
    "Below is simply the `count_words.nf` we worked with in our previous Nextflow workshop where we chain two tasks together: `sayHello` and direct its output (e.g., \"Hello Blueno\") to `countWords` to count the number of words in that output. \n",
    "\n",
    "###### count_words.nf\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input: \n",
    "    val name\n",
    "  output:\n",
    "    path \"hello.txt\"\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process countWords {\n",
    "  input: \n",
    "    path(file_in)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   \"\"\"\n",
    "   cat ${file_in}\n",
    "   wc -w ${file_in} | awk '{print \\$1}'\n",
    "   \"\"\" \n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name)) | view\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d04f0b",
   "metadata": {},
   "source": [
    "#### Step 4b. Conditional Logic\n",
    "\n",
    "Below we want to demonstrate how conditional logic works in Nextflow workflows. We will extend the previous `count_words.nf` to have two boolean flags in our pipline `--reverse` (reverse the words in our final output) and `--count_letters` (if the flag is set, count the letters insteads of words in the final output)\n",
    "\n",
    "##### Add `reverse` function in the body of the workflow:\n",
    "```bash\n",
    "params.reverse = false\n",
    "\n",
    "workflow {\n",
    "  hello_result = null\n",
    "\n",
    "  if (params.reverse) {\n",
    "    hello_result = reverse(sayHello(params.name))\n",
    "  } else {\n",
    "    hello_result = sayHello(params.name)\n",
    "  }\n",
    "\n",
    "  count(hello_result) | view\n",
    "}\n",
    "```\n",
    "\n",
    "##### Add `count_letters` function in the function of `count`:\n",
    "```bash\n",
    "params.count_letters = false\n",
    "... \n",
    "\n",
    "process count {\n",
    "  input: \n",
    "    path(file_in)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   if (params.count_letters) {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -m ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -w ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "##### Example Outputs:\n",
    "```bash\n",
    "nextflow run workflows/count_conditional.nf\n",
    "\n",
    "Hello World!\n",
    "2\n",
    "\n",
    "nextflow run workflows/count_conditional.nf --reverse\n",
    "!dlroW olleH\n",
    "2\n",
    "\n",
    "nextflow run workflows/count_conditional.nf --reverse --count_letters\n",
    "!dlroW olleH\n",
    "13\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b62fef",
   "metadata": {},
   "source": [
    "#### Step 4c. Parsing a Sample Sheet\n",
    "\n",
    "In real production Bioinformatics workflows, most of the time we cannot just get by with specifying a list of fasta raw or aligned reads. We also need to carry through all of the important metadata associated with a particular sample's sequencing data and provide their contexts to the analysis programs (e.g., Differential Gene Analysis where we need to differentiate between `treatment` and `control` groups; temporal analysis where we differentiate between the samples taken at different time points). \n",
    "\n",
    "We will demonstrate this concept by making a toy workflow that takes a samplesheet of documents and, according to a specification that we set for each document, counts either the number of letters or words of the document (determined by `count_letters` column).\n",
    "\n",
    "##### Make a simple samplesheet:\n",
    "Our toy sample sheet: \n",
    "\n",
    "```bash\n",
    "document,count_letters\n",
    "samplesheet/hello1.txt,F\n",
    "samplesheet/hello2.txt,T\n",
    "```\n",
    "\n",
    "##### Make a `documents_ch` as a channel of documents; parsed from the samplesheet above:\n",
    "\n",
    "Using `Channel.fromPath(params.samplesheet).splitCsv(header:true)`, We create a \"channel\" (or a \"stream\") of documents; where each entry is a tuple of both the document file itself and a boolean flag whether to count it by words or letters:\n",
    "`[ document, count_letters ]`.\n",
    "\n",
    "###### count_samplesheet.nf:\n",
    "```bash\n",
    "...\n",
    "// convert each row in the samplesheet into a tuple of File object and a Boolean of count_letter\n",
    "def get_document_info(LinkedHashMap sample) {\n",
    "    document  = sample.document ? file(sample.document, checkIfExists: true) : null\n",
    "    count_letters = (sample.count_letters == \"T\") || (sample.count_letters == \"true\") ? true : false   \n",
    "\n",
    "    return [ document, count_letters ]\n",
    "}\n",
    "...\n",
    "workflow {\n",
    "     // create a Channel of documents to count either by word or by letter, from the samplesheet\n",
    "     Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_document_info(it) }.set { documents_ch }\n",
    "            \n",
    "     // launch a sub-workflow of COUNT_DOCUMENT on each document\n",
    "     COUNT_DOCUMENT(documents_ch)\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "##### Define the sub-workflow `COUNT_DOCUMENT` which will process each individual document and count appropriately by its metadata flag:\n",
    "\n",
    "Instead of refering to a global variable `params.count_letters` as we did previously. \n",
    "\n",
    "We refer to the current metadata we are processing: `tuple file(file_in), val(count_letters)`; which is: `[(samplesheet/hello1.txt,F), (samplesheet/hello2.txt,T)]`. And for `T`, we will count by letters; and for `F`, by words.\n",
    "\n",
    "```bash\n",
    "process count_document {\n",
    "  input: \n",
    "    tuple file(file_in), val(count_letters)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   if (count_letters) {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -m ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -w ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "\n",
    "workflow COUNT_DOCUMENT {\n",
    "    take:\n",
    "        input_ch\n",
    "\n",
    "    main:\n",
    "        count_document(input_ch) | view\n",
    "}\n",
    "```\n",
    "\n",
    "##### Example Outputs:\n",
    "```bash\n",
    "N E X T F L O W  ~  version 23.04.2\n",
    "Launching `workflows/count_samplesheet.nf` [dreamy_fourier] DSL2 - revision: c7971cd787\n",
    "executor >  local (2)\n",
    "[3f/9df991] process > COUNT_DOCUMENT:count_document (1) [100%] 2 of 2 ✔\n",
    "Count Me (using Letters)24\n",
    "\n",
    "Count Me (using Words)4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65123a4",
   "metadata": {},
   "source": [
    "#### Step 4d. Asynchronous and synchronous job execution (or scatter-gather pattern)\n",
    "\n",
    "Scatter-gather pattern is an important pattern in pipelining. The concept of this is that we want to split (\"scatter\") out entries on a fasta file or rows in a samplesheet for parallel execution and on any downstream jobs asynchronously (on as many nodes as possible on `OSCAR`). \n",
    "\n",
    "Then in the end of pipeline, we would like to aggregate (\"gather\") these results; for example, \n",
    " - In RNASeq, to sum up or contrast the gene expressions of the `wild-type` replicates against the `treatment` replicates\n",
    " - In metagenomics workflows, to summarize the overall distribution or diversity of all the samples in a data-set.\n",
    " \n",
    " \n",
    "Continuing with our toy example of document counting, we now would like to demonstrate this concept by simply gathering all of the individual counts (whether by letter or by words) and summing all of them togethe to give an overall total count. \n",
    "\n",
    "\n",
    "###### Add a variable `count_result` which is an synchronous collection/`collect()` of all the scattered or asynchronous runs of `COUNT_DOCUMENT`:\n",
    "\n",
    "By defining a variable to hold onto `COUNT_DOCUMENT(documents_ch).collect()`, we are essentially assigning `count_results` as the gather-step's result placeholder. In other words, we will wait for all `COUNT_DOCUMENTS` to finish, and all individual document counts to be gathered into `count_results` and then run the final gather step of `sum_all_results(count_results)`. \n",
    "\n",
    "###### count_and_summarize.nf:\n",
    "```bash\n",
    "process sum_all_results {\n",
    "  input: \n",
    "    path (files)\n",
    "  output:\n",
    "    stdout\n",
    "\n",
    "  script:\n",
    "    \"\"\"\n",
    "    cat ${files} > combined.txt\n",
    "    awk '{ sum += \\$1 } END { print sum }' combined.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "    ... \n",
    "    // gather all of the results of each document counting sub-workflow\n",
    "    count_results = COUNT_DOCUMENT(documents_ch).collect()\n",
    "    \n",
    "    // and feed this collection of results into a summing function\n",
    "    sum_all_results(count_results) | view\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "##### Example Outputs:\n",
    "```bash\n",
    "N E X T F L O W  ~  version 23.04.2\n",
    "Launching `workflows/count_and_summarize.nf` [zen_baekeland] DSL2 - revision: 478f5774ff\n",
    "executor >  local (3)\n",
    "[12/8215ea] process > COUNT_DOCUMENT:count_document (1) [100%] 2 of 2 ✔\n",
    "[ff/fbd4b5] process > sum_all_results                   [100%] 1 of 1 ✔\n",
    "28\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a024d4",
   "metadata": {},
   "source": [
    "#### Step 4e. Putting it all together in a real-world RNASeq example\n",
    "\n",
    "We have learned all of the essential patterns involved in creating pipelines; now we can begin to put all of them together in a simple RNASeq pipeline. Below is the code for a full RNASeq pipeline, but don't worry too much about completely understanding all of the steps. \n",
    "\n",
    "Our purpose here is to refresh/review the pipeline patterns we just learned applied to a real world example. Each application of the patterns will be called out following the code. \n",
    "\n",
    "###### rnaseq_simple.nf:\n",
    "\n",
    "```bash\n",
    "workflow PROCESS_SAMPLE {\n",
    "    take:\n",
    "        input_ch\n",
    "        reference_genome\n",
    "\n",
    "    main:\n",
    "        fastqc(input_ch)\n",
    "\n",
    "        trimmed_reads = trimmomatic(input_ch)\n",
    "\n",
    "        fastqcs = fastqc2(trimmed_reads).collect()\n",
    "        fastqc_screens = fastq_screen(trimmed_reads).collect()\n",
    "\n",
    "        aligned_bams = star(trimmed_reads, reference_genome)\n",
    "        marked_duplicate_bams = mark_duplicate(aligned_bams)\n",
    "\n",
    "        qualimaps = qualimap(marked_duplicate_bams).collect()\n",
    "\n",
    "        htseq_counts = htseq_count(marked_duplicate_bams).collect()\n",
    "        feature_counts = feature_count(marked_duplicate_bams)\n",
    "\n",
    "        multiqc(fastqcs, fastqc_screens, qualimaps, htseq_counts)\n",
    "}\n",
    "\n",
    "// Function to resolve files\n",
    "def get_sample_info(LinkedHashMap sample) {\n",
    "    read1  = sample.read1 ? file(sample.read1, checkIfExists: true) : null\n",
    "    read2 = sample.read2 ? file(sample.read2, checkIfExists: true) : null\n",
    "\n",
    "    return [ sample.sample_id, read1, read2 ]\n",
    "}\n",
    "\n",
    "workflow {\n",
    "     reference_genome = params.reference_genome\n",
    "\n",
    "     if (params.reference_genome_fasta != \"\") {\n",
    "        reference_genome = build_star_index()\n",
    "     }\n",
    "\n",
    "     Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_sample_info(it) }.set { samples_ch }\n",
    "\n",
    "     PROCESS_SAMPLE (samples_ch, reference_genome)\n",
    "}\n",
    "```\n",
    "\n",
    "###### Pattern 1: Conditional Logic\n",
    "\n",
    "If `--reference_genome_fasta` is specified, we will build an index for this reference genome (so that our raw reads could be aligned against this index more efficiently)\n",
    "\n",
    "```bash\n",
    "if (params.reference_genome_fasta != \"\") {\n",
    "    reference_genome = build_star_index()\n",
    "}\n",
    "```\n",
    "\n",
    "###### Pattern 2: Parsing the Samplesheet\n",
    "We parse a samplesheet of RNASeq samples which will be `(sample_id, read1, read2)` and carry this metadata throughout the rest of the workflow.\n",
    "\n",
    "```bash\n",
    "\n",
    "// Function to resolve files\n",
    "def get_sample_info(LinkedHashMap sample) {\n",
    "    read1  = sample.read1 ? file(sample.read1, checkIfExists: true) : null\n",
    "    read2 = sample.read2 ? file(sample.read2, checkIfExists: true) : null\n",
    "\n",
    "    return [ sample.sample_id, read1, read2 ]\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_sample_info(it) }.set { samples_ch }\n",
    "PROCESS_SAMPLE (samples_ch, reference_genome)\n",
    "```\n",
    "\n",
    "###### Pattern 3: Scatter-Gather using `collect()`\n",
    "We will process asynchronously all of the tasks for each RNASeq sample such as:\n",
    "- QCing the read `fastqc`\n",
    "- Screening the read for contamination/sequencing artifacts `fastq_screen`\n",
    "- Aligning the reads and marking duplicates on the aligned reads `star` aligner and `mark_duplicate` tool\n",
    "- Count the features (individual gene expression) on the algined reads `htseq`\n",
    "\n",
    "Then finally, we gather all of these results into an awesome data visualization/summarization tool `multiqc`. \n",
    "\n",
    "```bash\n",
    "        fastqcs = fastqc2(trimmed_reads).collect()\n",
    "        fastqc_screens = fastq_screen(trimmed_reads).collect()\n",
    "\n",
    "        ...\n",
    "\n",
    "        qualimaps = qualimap(marked_duplicate_bams).collect()\n",
    "        htseq_counts = htseq_count(marked_duplicate_bams).collect()\n",
    "\n",
    "        multiqc(fastqcs, fastqc_screens, qualimaps, htseq_counts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe43b14",
   "metadata": {},
   "source": [
    "## A Few Closing Thoughts.....\n",
    "\n",
    "This workshop just provided an introductory overview of running Nextflow worklfows on OSCAR. There is much more customization that you can do and much more advanced things you can perform. Some of these are: \n",
    "\n",
    "* Resource allocation by rule (or analysis step) \n",
    "* Using a config.yaml file to handle your samples and how you iterate through files\n",
    "* Running pipelines that skip steps and start at a specific step (for example, pipelines that fail at a certain step, you may not want to repeat everything but instead pick up where you left off)\n",
    "* Handling log files within specific steps so that you get detailed output for each rule \n",
    "* And much more...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439c3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac3d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
