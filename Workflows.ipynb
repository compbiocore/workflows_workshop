{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4601bc",
   "metadata": {},
   "source": [
    "<h1><center>Basic Bioinformatics Workflows on OSCAR: Snakemake and Nextflow</center></h1>\n",
    "<p><center>Instructors: Ashok Ragavendran and Jordan Lawson</center>\n",
    " <center>Center for Computation and Visualization</center>\n",
    " <center>Center for Computational Biology of Human Disease - Computational Biology Core</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2fc35",
   "metadata": {},
   "source": [
    "## What is Snakemake and Nextflow?  \n",
    "\n",
    "Snakemake and Nextflow are workflow management tools that allow users to easily write data-intensive computational **pipelines**. These pipelines, or workflows as they are also called, have the following key features:\n",
    "\n",
    "- Sequential processing of files\n",
    "- Usually requires more than one tool\n",
    "- Multiple programming languages\n",
    "- Most times each sample is processed individually\n",
    "- Compute resource intensive\n",
    "  - Alignment could take 16 cpus, 60 Gb RAM, 4-24 hours, 30Gb of disk space per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6649f1",
   "metadata": {},
   "source": [
    "## Why do we care about these pipelines? \n",
    "\n",
    "### Reason 1: Reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06abc40",
   "metadata": {},
   "source": [
    "<img src=\"./img/reproduce.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb742d3",
   "metadata": {},
   "source": [
    "The journal Nature published a survey that found that more than 70% of researchers have tried and failed to reproduce another scientist's experiments. This trend is hugely problematic because we then can't trust the findings from many studies enough to use them to make data-driven decisions. In short, we need tools and standards that help address the reproducibility crisis in science! \n",
    "\n",
    "Pipelines created with Snakemake and Nextflow incorporate version-control and state-of-the-art software tools, known as containers, to manage all software dependencies and create stable environments that can be trusted to reproduce analyses reliably and accurately. \n",
    "\n",
    "***Reproducibility is important for producing trustworthy research!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bbec0",
   "metadata": {},
   "source": [
    "### Reason 2: Portability\n",
    "\n",
    "#### What if we need to perform analyses with more resources?\n",
    "\n",
    "This type of scenario would require us to move our analyses to a different environment, for example, a High Performance Computing (HPC) cluster environment. \n",
    "\n",
    "An important feature of Snakemake and Nextflow workflow management tools is that they enable users to easily scale any pipeline written on a personal computer to then run on an HPC cluster such as OSCAR, the HPC cluster we use at Brown University. So now we can run our pipelines using high performance resources without having to change workflow definitions or hard-code a pipeline to a specific setup. As a result, **the code stays constant** across varying infrastructures, thereby allowing portability, easy collaboration, and avoiding lock-in. \n",
    "\n",
    "***In short, we can easily move our multi-step analyses (i.e., pipelines) to any place we need them!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d46b76",
   "metadata": {},
   "source": [
    "## So Let's See How All This Works! \n",
    "\n",
    "### Our Starting Point\n",
    "\n",
    "Say we have samples from reduced representation bisulfite sequencing (RRBS data) that we need to process on OSCAR by performing the following set of actions: \n",
    "\n",
    "<img src=\"./img/workflow.png\" width=\"700\"/>\n",
    "\n",
    "<h2><center>What do you do??</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbe0a7",
   "metadata": {},
   "source": [
    "## A Naive Approach\n",
    "\n",
    "One solution would be to write a bunch of shell scripts that use various software tools to process the data in the ways we need. \n",
    "\n",
    "For example, if we need to run fastqc, trim galore, and then an alignment, we could just write a shell script for each step - so a total of 4 shell scripts in this case - where we have various inputs and outputs. This would look something as follows: \n",
    "\n",
    "**Script 1: Fastqc**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J rrbs_fastqc\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/Sequencing_Files/*fastq.gz`\n",
    "do\n",
    "align_dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs\" \n",
    "fastqc -o ${align_dir}/fastqc $sample\n",
    "done\n",
    "```\n",
    "\n",
    "**Script 2: Trimming** \n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J trimmomatic_update\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trim_galore/*_trimmed.fq.gz`\n",
    "do\n",
    "    dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic\"\n",
    "    base=$(basename $sample \"_trimmed.fq.gz\")\n",
    "    trimmomatic SE  -threads 8 -trimlog ${dir}/${base}_SE.log $sample ${dir}/${base}_tr.fq.gz ILLUMINACLIP:/gpfs/data/cbc/cbc_conda_v1/envs/cbc_conda/opt/trimmomatic-0.36/adapters/TruSeq3-SE.fa:2:30:5:6:true SLIDINGWINDOW:4:20 MINLEN:35\n",
    "done \n",
    "```\n",
    "\n",
    "\n",
    "**Script 3: Fastqc on trimmed reads**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -n 8\n",
    "#SBATCH -J retrim_fastqc_update\n",
    "#SBATCH --mem=16GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic/*_tr.fq.gz`\n",
    "do\n",
    "trim_qc_dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs\"\n",
    "fastqc -o ${trim_qc_dir}/trimmomatic_qc $sample\n",
    "done\n",
    "```\n",
    "\n",
    "**Script 4: Alignment**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 16\n",
    "#SBATCH -J bismark_align_update_redo\n",
    "#SBATCH --mem=160GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "#SBATCH --array=1-18\n",
    "#SBATCH -e /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/logs/bismark_align_%a_%A_%j.err\n",
    "#SBATCH -o /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/logs/bismark_align_%a_%A_%j.out\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "input=($(ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic/*_tr.fq.gz)) # using the round brackets indicates that this is a bash array\n",
    "bismark -o /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/alignments --bowtie2 --genome /gpfs/data/shared/databases/refchef_refs/S_scrofa/primary/bismark_index --un --pbat ${input[$((SLURM_ARRAY_TASK_ID -1))]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5d964",
   "metadata": {},
   "source": [
    "## Problems with the Naive Approach \n",
    "\n",
    "Using multiple shell scripts to create a makeshift pipeline will work, but it is **inefficient**, can **get complicated fast**, and there are a few **challenges you have to manage**, such as: \n",
    "\n",
    "* Making sure you have the appropriate software and all dependencies for each step in the analysis - this can be a lot to stay on top of if you have a pipeline with a lot of steps! (imagine a 10 step process)\n",
    "* **Portability!** Building and running on different machines is much more work\n",
    "* Specifying where your output will go \n",
    "* Calling in the appropriate input (which is often the output from a previous step) \n",
    "* Handling where log files go \n",
    "* More labor intensive - we have to stay on top of jobs and monitor when each step finishes and then run next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb9e7b",
   "metadata": {},
   "source": [
    "## A Smarter Approach: Using Workflow Managers! \n",
    "\n",
    "The solution for processing your data in a much more efficient manner that handles the aforementioned issues is workflow management tools, such as Snakemake and Nextflow. Let's now learn how to use Snakemake and Nextflow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7259fc",
   "metadata": {},
   "source": [
    "## Tutorial: Using workflow management tools on OSCAR \n",
    "\n",
    "Workflow management tools are software that allow you to write more efficient, portable computational pipelines. As a result, you are able to optimize your workflows while maintaining reproducibility and rigor. Note that there are many workflow management tools available to researchers, but the two tools we will focus on learning about today are **Snakemake** and **Nextflow**. Let's first start with Snakemake..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf3190",
   "metadata": {},
   "source": [
    "<img src=\"./img/snakemake.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece25d38",
   "metadata": {},
   "source": [
    "### Step 1: The Setup\n",
    "\n",
    "Let's first discuss setting up our environment on OSCAR so that we can get Snakemake up and running. There are a few ways to run Snakemake on OSCAR. For example, one could set up Snakemake through Conda environments or you could use singularity containers. The details of these tools (i.e., Conda environments and singularity containers) are beyond the scope of this workshop, but here are some helpful links to get you started learning more about these tools, should you be interested: \n",
    "\n",
    "<u>Conda and Conda environments:</u> \n",
    "\n",
    "https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/environments.html <br>\n",
    "\n",
    "https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html <br>\n",
    "\n",
    "\n",
    "<u> Singularity containers: </u> \n",
    "\n",
    "https://sylabs.io/guides/3.5/user-guide/introduction.html <br>\n",
    "\n",
    "https://blogs.iu.edu/ncgas/2021/04/29/a-quick-intro-to-singularity-containers/ <br>\n",
    "\n",
    "\n",
    "For this workshop, the specific approach we are going to take is to set up a Python virtual environment (essentially just an isolated environment containing the few pieces of software we need to get up and running) and then within this environment we tell Snakemake to download and run specific pre-built singularity containers for each of the different data processing steps (known as Snakemake rules). Singularity containers are just computer programs a.k.a virtual machines that encapsulate all the software needed for a workflow and thus enable reproducibility. They are widely used on HPC systems because of their increased security relative to other software options.\n",
    "\n",
    "So now let's get started with our set up. First, ssh into OSCAR and wherever you like (and have enough space for storage) create a folder called **snakemake**. To do this, simply type ```mkdir snakemake```. \n",
    "\n",
    "Now enter this folder using ```cd /path/to/snakemake``` and inside it we will set up our virtual environment so that we can have access to and run Snakemake software on OSCAR. To set up our virtual environment, we use the following script: \n",
    "\n",
    "<br> \n",
    "\n",
    "```\n",
    "module load python/3.9.0    # load version of Python needed \n",
    "virtualenv snakemake_env     # create environment \n",
    "source snakemake_env/bin/activate    # activate environment\n",
    "pip3 install snakemake    # install snakemake using pip\n",
    "deactivate    # deactivate and exit, we will use again later!\n",
    "```\n",
    "    \n",
    "**Note:** Save and run this script in your snakemake folder using ```bash env.sh```. This will create a snakemake_env folder in your project directory. \n",
    "\n",
    "After this code has been run, you can now at any point type ```source /path/to/environment/environment_folder_name/bin/activate``` and you will enter an environment that has snakemake software installed and ready to be called for use. Now let's move onto setting up the specific pieces of input that Snakemake needs to run on OSCAR, our HPC cluster. \n",
    "\n",
    "**At this point, I am also going to open my terminal on Open OnDemand (OOD) so that I can walk you through and show you how each of these steps and files below look. Feel free to open your terminal as well and follow along. To do so, you can go to OOD at https://ood.ccv.brown.edu/pun/sys/dashboard and under the Clusters tab at the top select >_OSCAR Shell Access.**\n",
    "\n",
    "### Step 2: Creating the Snakefile \n",
    "\n",
    "To start using Snakemake on any platform, the first thing one must do is create a **Snakefile**. A Snakefile is a file that defines a Snakemake workflow in terms of rules that are to be carried out in a specific order to complete a pipeline. This file determines the entire flow of your data analysis pipeline, specifying the rules to be carried out and their respective inputs and outputs. We can name this file whatever we like, as long as we tell our Snakemake program where to find it; however, by convention, we usually name this file Snakefile (with no extension!). \n",
    "\n",
    "I create a file called Snakefile and store this file in my newly created **snakemake** folder. Drawing on the same RRBS example we started with, the Snakefile would be: \n",
    "\n",
    "**Snakefile**\n",
    "\n",
    "```python\n",
    "## Snakefile for BootCamp ##\n",
    "\n",
    "# Specify configuration file to use - optional\n",
    "# configfile: \"/path/to/file/config.yaml\"\n",
    "\n",
    "# Define sample to iterate across data \n",
    "sample=[\"sample_1\", \"sample_2\", \"sample_3\", \"sample_4\"]\n",
    "\n",
    "# Create rules\n",
    "rule all:\n",
    "    message: \"All done....!\"\n",
    "    input:\n",
    "        expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_trimmed.fq.gz\", sample=sample),\n",
    "        expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}.fastq.gz_trimming_report.txt\", sample=sample),\n",
    "        expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_bismark_bt2_pe.bam\", sample=sample),\n",
    "        expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_bismark_bt2_PE_report.txt\", sample=sample),\n",
    "        expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_bismark_bt2_pe.nucleotide_stats.txt\", sample=sample)\n",
    "\n",
    "\n",
    "rule fastqc:\n",
    "    message: \"Running FastQC...\"\n",
    "    input:\n",
    "        rawread=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/data_new/{sample}.fq.gz\", sample=sample) \n",
    "    output:\n",
    "        zip=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/fastqc/{sample}.zip\", sample=sample), \n",
    "        html=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/fastqc/{sample}.html\", sample=sample)\n",
    "    singularity: \"library://ftabaro/default/methylsnake\" \n",
    "    shell: \"fastqc {input.rawread} -o /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/fastqc\"\n",
    "\n",
    "rule trim:\n",
    "   message: \"Performing reads trimming...\" \n",
    "   input:\n",
    "       rawreads=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/data_new/{sample}.fq.gz\", sample=sample)\n",
    "   output:\n",
    "       trimmed=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_trimmed.fq.gz\", sample=sample),\n",
    "       report=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}.fastq.gz_trimming_report.txt\", sample=sample)\n",
    "   params:  \n",
    "     quality_filter_value=\"22\",\n",
    "   threads: 4\n",
    "   singularity: \"library://ftabaro/default/methylsnake\"\n",
    "   shell: \"trim_galore --quality {params.quality_filter_value} --phred33 --output_dir /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim --gzip --rrbs --fastqc --cores {threads} {input.rawreads}\" \n",
    "\n",
    "rule bismark_align:\n",
    "    message: \"Performing alignment...\"\n",
    "    input:\n",
    "        trimreads=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_trimmed.fq.gz\", sample=sample)\n",
    "    output: \n",
    "      bam=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_bismark_bt2_pe.bam\", sample=sample),\n",
    "      bisreport=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_bismark_bt2_PE_report.txt\", sample=sample),\n",
    "      stats=expand(\"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/trim/{sample}_bismark_bt2_pe.nucleotide_stats.txt\", sample=sample)\n",
    "    threads: 4\n",
    "    singularity: \"library://ftabaro/default/methylsnake\"\n",
    "    shell: \"bismark -o /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/alignment --bowtie2 --genome /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/index --un --pbat {input.trimreads}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad8e88",
   "metadata": {},
   "source": [
    "### Step 3: Telling Snakemake how to run on the HPC Cluster  \n",
    "\n",
    "After defining the rules and analysis steps for our workflow via the Snakefile, we now must set up the workflow to be compatible with our HPC cluster, telling it how to assign resources to each rule. This is done by creating a yaml file called **cluster.yaml** (really it can be called anything, as long as the extension is .yaml). This file can be stored anywhere, as long as you specify where it can be found by the Snakemake program when it's run (will see this later). But for simplicity, it's best to just create and save it in the same place as your Snakefile. We specify the cluster.yaml file as follows: \n",
    "\n",
    "**cluster.yaml**\n",
    "\n",
    "```yaml\n",
    "__default__:\n",
    "  partition: \"batch\"\n",
    "  cpus: \"1\"\n",
    "  time: 60\n",
    "  mem: \"4g\"\n",
    "  email: jordan_lawson@brown.edu  \n",
    "  email_type: \"ALL\"\n",
    "```\n",
    "\n",
    "In the above, we create a default specification, where each rule, by default, will get this resource allocation when run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da9821",
   "metadata": {},
   "source": [
    "### Step 4: Bringing it all together with Shell Scripting \n",
    "\n",
    "Now here is the step where we tie everything together, running our Snakefile and cluster specification all with one script, a shell script. To run everything on OSCAR, we can use the following script: \n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "##############################\n",
    "#                             # \n",
    "#       Snakemake             #\n",
    "#                             #\n",
    "###############################\n",
    "\n",
    "##### 1.) Job Sumbission Options ######\n",
    "\n",
    "# Change these as needed \n",
    "\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -n 2\n",
    "#SBATCH -J snake_test\n",
    "#SBATCH --mem=16GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "###### 2.) Run snakemake #####\n",
    "\n",
    "# Activate virtual environment \n",
    "source snakemake_env/bin/activate\n",
    "\n",
    "# Run snakemake - note you will need a cluster.yaml file as this command references one!\n",
    "snakemake --use-singularity --singularity-args \"-B /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/:/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/\" -s /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/Snakefile --cluster-config /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/snakemake/cluster.yaml --latency-wait 60 --cluster 'sbatch -t {cluster.time} --mem={cluster.mem} -c {cluster.cpus} --mail-type {cluster.email_type} --mail-user {cluster.email}' -j 10\n",
    "```\n",
    "\n",
    "I save this script as **snakemake.sh** and place it in the snakemake folder with the other files I recently created. Once we have this with everything else, we can run everything on the cluster by typing ```sbatch snakemake.sh```\n",
    "\n",
    "Note in the above that we are running the snakemake program with the Snakefile and cluster.yaml files we specified. Some other important things: \n",
    "\n",
    "- your log file for slurm will automtatically be placed in the same directory that you ran the snakemake.sh script from. However, you can also get more detailed log files for each snakemake rule and its respective outputs by using the ```log:``` command followed by the path you want the logs files to go to in each of the rules found within the Snakefile \n",
    "\n",
    "- we are using the --use-singularity argument in the above shell script to allow for Snakemake to use a singularity container when running pipelines; this is highly recommended for reproducibility. \n",
    "\n",
    "- when using the singularity argument, **it is very, very important** that you include the ```--singularity-args \"-B folder_to_mount:source_destination\"``` argument or else your pipeline will fail to recognize any inputs and outputs you are specifying in the workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325c7ef",
   "metadata": {},
   "source": [
    "## Next up.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372c107",
   "metadata": {},
   "source": [
    "<img src=\"./img/nextflow.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872e2d",
   "metadata": {},
   "source": [
    "### Step 1: The Setup\n",
    "\n",
    "Once again, our starting point is setting up our environment on OSCAR so that we can get Nextflow up and running. Like with Snakemake, there are a few ways to run Nextflow on OSCAR, but we will be using a similar approach to the one we used with Snakemake, setting up a Python virtual environment first so that we have access to the Nextflow program and then running pipelines from there.  \n",
    "\n",
    "**Nextflow vs Snakemake:** Nextflow is very much like Snakemake in that they both serve the same need, so which one you use is largely up to you and is preference-based. However, one very notable difference is that Nextflow has an open-source, community built repository of bioinformatics pipelines that you can easily download and use for your own data processing needs so that you don't have to write your own pipelines (but you can if you want to!). This resource is called **nf-core** and is a community effort to collect a curated set of pipelines built using Nextflow. This is nice because it already has many of the workflows and analyses that computational biologists use and so we don't have to waste our efforts and time re-inventing the wheel! Given these advantages and our focus on bioinformatics, today's example will illustrate how to use Nextflow on OSCAR with nf-core. Note, however, that much of what we will learn today about using Nextflow with nf-core still applies to developing and running your own workflows with Nextflow without the use of nf-core. To learn more about nf-core, you can visit its homepage here: \n",
    "\n",
    "https://nf-co.re\n",
    "\n",
    "So now let's get started with our set up. First, ssh into OSCAR and wherever you like (and have enough space for storage) create a folder called **nextflow**. To do this, simply type ```mkdir nextflow```. \n",
    "\n",
    "Now enter this folder using ```cd /path/to/nextflow``` and inside it we will set up our virtual environment so that we can have access to and run Nextflow software on OSCAR. To set up our virtual environment, we use the following script: \n",
    "\n",
    "**env.sh**\n",
    "\n",
    "```\n",
    "# Create virutal environment \n",
    "virtualenv nextflow_env \n",
    "source ~/nextflow_env/bin/activate \n",
    "wget -qO- https://get.nextflow.io | bash\n",
    "# Make executable \n",
    "chmod +x nextflow\n",
    "mv nextflow $HOME/nextflow_env/bin\n",
    "deactivate\n",
    "# You can now run nextflow in bash script using `source path/to/nextflow_env/bin/activate` \n",
    "```\n",
    "\n",
    "**Note:** Save and run this script in your nextflow folder using ```bash env.sh```. This will create a nextflow_env folder in your project directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc5e51",
   "metadata": {},
   "source": [
    "###  Step 2: Configure Nextflow for HPC cluster \n",
    "\n",
    "Next, we tell Nextflow how to run pipelines on our HPC cluster by creating a **.config** file. We will call this .config file **nextflow.config** and store it inside or newly created nextflow folder. We create the file as follows: \n",
    "\n",
    "```\n",
    "params {\n",
    "  config_profile_description = 'CBC cluster profile'\n",
    "  config_profile_contact = 'Jordan Lawson (jordan_lawson@brown.edu)'\n",
    "  singularity_cache_dir = \"/users/jlawson9/scratch\"\n",
    "  max_memory = 192.GB\n",
    "  max_cpus = 56\n",
    "  max_time = 24.h\n",
    "}\n",
    "singularity {\n",
    "  enabled = true\n",
    "  cacheDir=params.singularity_cache_dir\n",
    "  autoMounts = true\n",
    "}\n",
    "process {\n",
    "  executor = 'slurm'\n",
    "  clusterOptions = '-p batch'\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c92202",
   "metadata": {},
   "source": [
    "### Step 3: Giving nf-core Access to GitHub \n",
    "\n",
    "We are using Nextflow with nf-core, and nf-core downloads software using Git and GitHub. Therefore, we have to enable access to GitHub to ensure that we can fetch the right tools when using Nextflow. To do this, we have to create what's called a **scm** file (no file extension!) in our nextflow folder. This file specifies the account to use with the Nextflow software and its respective credentials. We generate this file as follows: \n",
    "\n",
    "```\n",
    "providers {\n",
    "\n",
    "  github {\n",
    "    user = 'me'\n",
    "    password = 'my-personal-access-token'\n",
    "  }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "**Very important:** Once this file is created, we have to move it to the following location: **$HOME/.nextflow/scm** <br>  \n",
    "To do this, type ```mv scm $HOME/.nextflow/scm```\n",
    "\n",
    "Once this has been set up, we are ready to get to the last step and start using Nextflow on OSCAR! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4c1e",
   "metadata": {},
   "source": [
    "### Step 4: Selecting An nf-core Pipeline\n",
    "\n",
    "nf-core has many analysis pipelines we can use and so we need to identify the specific pipeline that is appropriate for our needs. We are once again using the RRBS example that we started with, so we need a pipeline that is appropriate to use for processing RRBS data. Heading over to https://nf-co.re/pipelines we can see that the **methylseq** pipeline will work for our data. We can view the details of this pipeline, such as all of its arguments and the steps it performs, by cliking on its link or visiting here: https://nf-co.re/methylseq \n",
    "\n",
    "**Note:** Visitng the pipeline's page and reviewing the pipeline's documentation is important because we need to know what arguments to use to make it run correctly. Once we've reviewed this and got an idea of how we need to run things, we can move to the final step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60791962",
   "metadata": {},
   "source": [
    "### Step 4: Shell Script To Bring It All Together\n",
    "\n",
    "The last step is to create a shell script to run Nextflow with the desired nf-core pipeline on OSCAR. To do this, we create a bash script in our nextflow folder called **nextflow.sh**. This script is written as follows: \n",
    "\n",
    "**nextflow.sh**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "###############################\n",
    "#                             #\n",
    "#   Nextflow for BootCamp     #\n",
    "#                             #\n",
    "###############################\n",
    "\n",
    "##### 1.) Job Sumbission Options ######\n",
    "\n",
    "# Can change/add resources as needed \n",
    "\n",
    "#SBATCH -n 4\n",
    "#SBATCH --mem 8G\n",
    "#SBATCH -t 2:00:00\n",
    "\n",
    "# Logs\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu  \n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=%x-%j.out\n",
    "\n",
    "##### 2.) Run Nextflow #####\n",
    "\n",
    "# Activate environment previosuly created \n",
    "source /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/nextflow/nextflow_env/bin/activate\n",
    "\n",
    "# Run nextflow \n",
    "nextflow run nf-core/methylseq -profile singularity --input \"/gpfs/data/ccvstaff/jlawson9/bootcamp_2022/data_new/*.fq.gz\" --single_end --genome Sscrofa10.2 -c /gpfs/data/ccvstaff/jlawson9/bootcamp_2022/nextflow/nextflow.config --outdir $HOME/scratch\n",
    "```\n",
    "\n",
    "Once we have this file with everything else, we can run everything on the cluster by typing ```sbatch nextflow.sh```\n",
    "\n",
    "Note in the above that we are running the nextflow program with the methylseq pipeline, as specified in the ```nf-core/methylseq``` argument that comes right after the ```nextflow run``` command. This tells nextflow to fetch and download the methylseq pipeline from nf-core. Some more important notes: \n",
    "\n",
    "- your log file for slurm will automtatically be placed in the same directory that you ran the nextflow.sh script from. You can control where log files go using the ```-log``` flag followed by the path you wish to store the logs at. \n",
    "\n",
    "- the -c flag handles the nextflow.config file we created\n",
    "\n",
    "- we are using the -profile singularity argument in the above shell script so that Nextflow uses a singularity container when the running pipeline; this is highly recommended for reproducibility. \n",
    "\n",
    "- the other arguments are bioinformatics specific and are there to make sure the pipeline runs according to our needs; they were found by visitng the pipeline's documentation page at: https://nf-co.re/methylseq (which you can get to from the nf-core homepage). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe43b14",
   "metadata": {},
   "source": [
    "## A Few Closing Thoughts.....\n",
    "\n",
    "This workshop just provided an introductory overview of running worklfow management tools on OSCAR. There is much more customization that you can do and much more advanced things you can perform. Some of these are: \n",
    "\n",
    "* Resource allocation by rule (or analysis step) \n",
    "* Using a config.yaml file to handle your samples and how you iterate through files\n",
    "* Running pipelines that skip steps and start at a specific step (for example, pipelines that fail at a certain step, you may not want to repeat everything but instead pick up where you left off)\n",
    "* Handling log files within specific steps so that you get detailed output for each rule \n",
    "* And much more...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
