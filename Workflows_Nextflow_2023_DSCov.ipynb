{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4601bc",
   "metadata": {},
   "source": [
    "<h1><center>Building Reproducible and Scalable Workflows on OSCAR: Nextflow</center></h1>\n",
    "<p><center>Presenter: Paul Cao</center>\n",
    " <center>Center for Computation and Visualization</center>\n",
    " <center>Center for Computational Biology of Human Disease - Computational Biology Core</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfe5ec",
   "metadata": {},
   "source": [
    "Resources for help @brown <br> \n",
    "\n",
    "COBRE CBHD Computational Biology Core\n",
    "- Office hours\n",
    "- https://cbc.brown.edu\n",
    "- slack channel on ccv-share\n",
    "- cbc-help@brown.edu <br>\n",
    "\n",
    "Center for Computation and Visualization\n",
    "- Office hours\n",
    "- https://ccv.brown.edu\n",
    "- ccv-share slack channel\n",
    "- https://docs.ccv.brown.edu\n",
    "- support@ccv.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2fc35",
   "metadata": {},
   "source": [
    "## What is Nextflow?  \n",
    "\n",
    "Nextflow is workflow management tools that allow users to easily write data-intensive computational **pipelines**. These pipelines, or workflows as they are also called, have the following key features:\n",
    "\n",
    "- Sequential processing of files\n",
    "- Usually requires more than one tool\n",
    "- Multiple programming languages\n",
    "- Most times each sample is processed individually\n",
    "- Compute resource intensive\n",
    "  - Alignment could take 16 cpus, 60 Gb RAM, 4-24 hours, 30Gb of disk space per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6649f1",
   "metadata": {},
   "source": [
    "## Why do we care about these pipelines? \n",
    "\n",
    "### Reason 1: Reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06abc40",
   "metadata": {},
   "source": [
    "<img src=\"./img/reproduce.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb742d3",
   "metadata": {},
   "source": [
    "The journal Nature published a survey that found that more than 70% of researchers have tried and failed to reproduce another scientist's experiments. This trend is hugely problematic because we then can't trust the findings from many studies enough to use them to make data-driven decisions. In short, we need tools and standards that help address the reproducibility crisis in science! \n",
    "\n",
    "Pipelines created with Snakemake and Nextflow incorporate version-control and state-of-the-art software tools, known as containers, to manage all software dependencies and create stable environments that can be trusted to reproduce analyses reliably and accurately. \n",
    "\n",
    "***Reproducibility is important for producing trustworthy research!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bbec0",
   "metadata": {},
   "source": [
    "### Reason 2: Portability\n",
    "\n",
    "#### What if we need to perform analyses with more resources?\n",
    "\n",
    "This type of scenario would require us to move our analyses to a different environment, for example, a High Performance Computing (HPC) cluster environment. \n",
    "\n",
    "An important feature of Snakemake and Nextflow workflow management tools is that they enable users to easily scale any pipeline written on a personal computer to then run on an HPC cluster such as OSCAR, the HPC cluster we use at Brown University. So now we can run our pipelines using high performance resources without having to change workflow definitions or hard-code a pipeline to a specific setup. As a result, **the code stays constant** across varying infrastructures, thereby allowing portability, easy collaboration, and avoiding lock-in. \n",
    "\n",
    "***In short, we can easily move our multi-step analyses (i.e., pipelines) to any place we need them!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d46b76",
   "metadata": {},
   "source": [
    "## So Let's See How All This Works! \n",
    "\n",
    "### Our Starting Point\n",
    "\n",
    "Say we have samples from RNASeq that we need to process on OSCAR by performing the following set of actions: \n",
    "\n",
    "<img src=\"./img/rnaseq_flowchart.png\" width=\"700\"/>\n",
    "\n",
    "<h2><center>What do you do??</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbe0a7",
   "metadata": {},
   "source": [
    "## A Naive Approach\n",
    "\n",
    "One solution would be to write a bunch of shell scripts that use various software tools to process the data in the ways we need. \n",
    "\n",
    "For example, if we need to run fastqc, trim galore, and then an alignment, we could just write a shell script for each step - so a total of 4 shell scripts in this case - where we have various inputs and outputs. This would look something as follows: \n",
    "\n",
    "**Script 1: Fastqc**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J rrbs_fastqc\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/Sequencing_Files/*fastq.gz`\n",
    "do\n",
    "align_dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs\" \n",
    "fastqc -o ${align_dir}/fastqc $sample\n",
    "done\n",
    "```\n",
    "\n",
    "**Script 2: Trimming** \n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J trimmomatic_update\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trim_galore/*_trimmed.fq.gz`\n",
    "do\n",
    "    dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic\"\n",
    "    base=$(basename $sample \"_trimmed.fq.gz\")\n",
    "    trimmomatic SE  -threads 8 -trimlog ${dir}/${base}_SE.log $sample ${dir}/${base}_tr.fq.gz ILLUMINACLIP:/gpfs/data/cbc/cbc_conda_v1/envs/cbc_conda/opt/trimmomatic-0.36/adapters/TruSeq3-SE.fa:2:30:5:6:true SLIDINGWINDOW:4:20 MINLEN:35\n",
    "done \n",
    "```\n",
    "\n",
    "\n",
    "**Script 3: Fastqc on trimmed reads**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -n 8\n",
    "#SBATCH -J retrim_fastqc_update\n",
    "#SBATCH --mem=16GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic/*_tr.fq.gz`\n",
    "do\n",
    "trim_qc_dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs\"\n",
    "fastqc -o ${trim_qc_dir}/trimmomatic_qc $sample\n",
    "done\n",
    "```\n",
    "\n",
    "**Script 4: Alignment**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 16\n",
    "#SBATCH -J bismark_align_update_redo\n",
    "#SBATCH --mem=160GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "#SBATCH --array=1-18\n",
    "#SBATCH -e /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/logs/bismark_align_%a_%A_%j.err\n",
    "#SBATCH -o /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/logs/bismark_align_%a_%A_%j.out\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "input=($(ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic/*_tr.fq.gz)) # using the round brackets indicates that this is a bash array\n",
    "bismark -o /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/alignments --bowtie2 --genome /gpfs/data/shared/databases/refchef_refs/S_scrofa/primary/bismark_index --un --pbat ${input[$((SLURM_ARRAY_TASK_ID -1))]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5d964",
   "metadata": {},
   "source": [
    "## Problems with the Naive Approach \n",
    "\n",
    "Using multiple shell scripts to create a makeshift pipeline will work, but it is **inefficient**, can **get complicated fast**, and there are a few **challenges you have to manage**, such as: \n",
    "\n",
    "* Making sure you have the appropriate software and all dependencies for each step in the analysis - this can be a lot to stay on top of if you have a pipeline with a lot of steps! (imagine a 10 step process)\n",
    "* **Portability!** Building and running on different machines is much more work\n",
    "* Specifying where your output will go \n",
    "* Calling in the appropriate input (which is often the output from a previous step) \n",
    "* Handling where log files go \n",
    "* More labor intensive - we have to stay on top of jobs and monitor when each step finishes and then run next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb9e7b",
   "metadata": {},
   "source": [
    "## A Smarter Approach: Using Workflow Managers! \n",
    "\n",
    "The solution for processing your data in a much more efficient manner that handles the aforementioned issues is workflow management tools, such as Snakemake and Nextflow. Let's now learn how to use Snakemake and Nextflow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7259fc",
   "metadata": {},
   "source": [
    "## Tutorial: Using workflow management tools on OSCAR \n",
    "\n",
    "Workflow management tools are software that allow you to write more efficient, portable computational pipelines. As a result, you are able to optimize your workflows while maintaining reproducibility and rigor. Note that there are many workflow management tools available to researchers, but the two tools we will focus on learning about today are **Snakemake** and **Nextflow**. Let's first start with Nextflow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09894a3",
   "metadata": {},
   "source": [
    "### Step 1: The Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdc919",
   "metadata": {},
   "source": [
    "Let's first discuss setting up our environment on OSCAR so that we can get Snakemake up and running. \n",
    "\n",
    "**At this point, I am going to open my terminal on Open OnDemand (OOD) so that I can walk you through and show you how each of these steps and files below look. Feel free to open your terminal as well and follow along. To do so, you can go to OOD at https://ood.ccv.brown.edu and under the Clusters tab at the top select the >_OSCAR Shell Access option. All files used today can be found on GitHub in the folder at: https://github.com/compbiocore/workflows_on_OSCAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32339e10",
   "metadata": {},
   "source": [
    "#### Step 1a: Set up Nextflow Configuration Script using `compbiocore/workflows_on_OSCAR`:\n",
    "\n",
    "```bash\n",
    "[pcao5@node1322 ~]$ cd ~/\n",
    "[pcao5@node1322 ~]$ git clone https://github.com/compbiocore/workflows_on_OSCAR.git\n",
    "[pcao5@node1322 ~]$ git clone https://github.com/compbiocore/workflows_workshop.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944acb9",
   "metadata": {},
   "source": [
    "#### Step 1b: Install `compbiocore/workflows_on_OSCAR` package:\n",
    "\n",
    "```bash\n",
    "bash ~/workflows_on_OSCAR/install_me/install.sh && source ~/.bashrc\n",
    "```\n",
    "\n",
    "\n",
    "For the 1st installation prompt, input `NextFlow`:\n",
    "\n",
    "```bash\n",
    "Welcome to a program designed to help you easily set up and run workflow management systems on OSCAR!\n",
    "\n",
    "Please type which software you wish to use: Nextflow or Snakemake? Nextflow\n",
    "```\n",
    "\n",
    "For the 2nd installation prompt, input your GitHub username (e.g., `paulcao-brown`):\n",
    "\n",
    "```bash\n",
    "Nextflow software detected, initializing configuration...\n",
    "What is your GitHub user name? paulcao-brown\n",
    "What is your GitHub token (we will keep this secret) - [Hit Enter when Done]?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d19eb",
   "metadata": {},
   "source": [
    "#### Step 1c: Create a new GitHub Token and enter it:\n",
    "<img src=\"./img/GBGDQhY.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109d512",
   "metadata": {},
   "source": [
    "#### Step 1d: Complete the Installation \n",
    "\n",
    "\n",
    "```bash\n",
    "Currently the Nextflow default for HPC resources is: memory = 5.GB time = 2.h cpus = 2 \n",
    "Do you want to change these default resources for your Nextflow pipeline [Yes or No]? No\n",
    "Keeping defaults!\n",
    "\n",
    "OUTPUT MESSAGE:\n",
    "\n",
    "                ******************************************************************\n",
    "                 NEXTFLOW is now set up and configured and ready to run on OSCAR!\n",
    "                ******************************************************************\n",
    "                \n",
    "\n",
    "Your default resources for Nextflow are: memory = 5.GB time = 2.h cpus = 2 \n",
    "\n",
    "\n",
    "                To further customize your pipeline for efficiency, you can enter the following \n",
    "                label '<LabelName>' options right within processes in your Nextflow .nf script:\n",
    "                1. label 'OSCAR_small_job' (comes with memory = 4 GB, time = 1 hour, cpus = 1)\n",
    "                2. label 'OSCAR_medium_job' (comes with memory = 8 GB, time = 16 hours, cpus = 2)\n",
    "                3. label 'OSCAR_large_job' (comes with memory = 16 GB, time = 24 hours, cpus = 2)\n",
    "                \n",
    "\n",
    "README:\n",
    "\n",
    "Please see https://github.com/compbiocore/workflows_on_OSCAR for further details on how to add the above label options to your workflow.\n",
    "\n",
    "Note the setup is designed such that pipelines downloaded from nf-core with their own resource specs within the .nf script will override your defaults.\n",
    "\n",
    "To run Nextflow commands, you must first type and run the nextflow_start command.\n",
    "\n",
    "To further learn how to easily run your Nextflow pipelines on OSCAR, use the Nextflow template shell script located in your ~/nextflow_setup directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2bd29",
   "metadata": {},
   "source": [
    "### Step 2: Run a 'Hello World' Example\n",
    "\n",
    "#### 2a. hello_world.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    stdout\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!'\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  sayHello(params.name) | view\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2b. Launch the Workflow:\n",
    "```bash\n",
    "cd workflows_workshop/workflows\n",
    "nextflow run hello_world.nf\n",
    "```\n",
    "\n",
    "#### 2c. Workflow Output:\n",
    "![](./img/8bNSCPv.png)\n",
    "\n",
    "#### 2d. Launch the Workflow (Custom Parameter):\n",
    "```bash\n",
    "nextflow run hello_world.nf --name \"Bleuno Bear\"\n",
    "```\n",
    "\n",
    "#### 2e. Workflow Output (Custom Parameter):\n",
    "![](https://i.imgur.com/YpjvNoe.png)\n",
    "\n",
    "#### 2f. Inspect Underneath the Hood the Command (e.g.,`52/992fb7`):\n",
    "```bash\n",
    "cd work/52/992fb76ea549e001e25b749b615f66/\n",
    "ls -la\n",
    "```\n",
    "\n",
    "##### Nextflow Generated Commands:\n",
    "\n",
    "![](./img/JK1Ly1s.png)\n",
    "\n",
    "\n",
    "##### The Actual Command Run:\n",
    "```bash\n",
    "cat .command.sh\n",
    "\n",
    "#!/bin/bash -ue\n",
    "echo 'Hello Blueno Bear!'\n",
    "```\n",
    "\n",
    "##### Slurm Wrapper Command:\n",
    "```bash \n",
    "cat .command.run\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -J nf-sayHello\n",
    "#SBATCH -o /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.log\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --signal B:USR2@30\n",
    "#SBATCH -c 2\n",
    "#SBATCH -t 02:00:00\n",
    "#SBATCH --mem 5120M\n",
    "#SBATCH -p batch\n",
    "NXF_CHDIR=/gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66\n",
    "# NEXTFLOW TASK: sayHello\n",
    "...\n",
    "/bin/bash -ue /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.sh &\n",
    "...\n",
    "```\n",
    "\n",
    "##### Standard Out/Error:\n",
    "```bash \n",
    "cat .command.log\n",
    "## SLURM PROLOG ###############################################################\n",
    "##    Job ID : 10159219\n",
    "##  Job Name : nf-sayHello\n",
    "##  Nodelist : node1322\n",
    "##      CPUs : 4\n",
    "##  Mem/Node : 5120 MB\n",
    "## Directory : /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66\n",
    "##   Job Started : Wed Jun  7 10:34:20 EDT 2023\n",
    "###############################################################################\n",
    "\n",
    "Hello Blueno Bear!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5b85e",
   "metadata": {},
   "source": [
    "### Step 3: Run a 'Word Count' Example (2 Step Workflow) \n",
    "\n",
    "#### 3a. word_count.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    path \"hello.txt\"\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process countWords {\n",
    "  input:\n",
    "    path(file_in)\n",
    "  output:\n",
    "    stdout\n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "   cat ${file_in}\n",
    "   wc -w ${file_in} | awk '{print \\$1}'\n",
    "   \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name)) | view\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3b. Launch the Workflow:\n",
    "```bash\n",
    "nextflow run count_words.nf --name \"Blueno Bear\"\n",
    "```\n",
    "\n",
    "#### 3c. Workflow Output:\n",
    "![](./img/649M0u8.png)\n",
    "\n",
    "In the next iteration, we want to save the word count to a text file; so we add to `word_count.nf` the following lines:\n",
    "- add this directive `publishDir \"${params.out_dir}/\", mode: 'copy'` to `countWords`\n",
    "- and change the `countWords` function to redirect output to `wc -w ${file_in} | awk '{print \\$1}' > count_words.txt`\n",
    "\n",
    "#### 3d. count_words_and_save.nf\n",
    "\n",
    "```bash\n",
    "!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    path \"hello.txt\"\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process countWords {\n",
    "  input:\n",
    "    path(file_in)\n",
    "  output:\n",
    "    path(\"count_words.txt\")\n",
    "\n",
    "  publishDir \"${params.out_dir}/\", mode: 'copy'\n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "   wc -w ${file_in} | awk '{print \\$1}' > count_words.txt\n",
    "   \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name))\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3e. Launch the Workflow\n",
    "```bash\n",
    "nextflow run count_words_and_save.nf --name \"Blueno Bear\" --out_dir words_out\n",
    "```\n",
    "\n",
    "##### Workflow Output (Custom Parameter):\n",
    "![](./img/nrrpAGV.png)\n",
    "\n",
    "##### Inspect the output directory `words_out`:\n",
    "```bash\n",
    "cat words_out/count_words.txt\n",
    "\n",
    "3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66093a86",
   "metadata": {},
   "source": [
    "### Step 4. Conditional Logic\n",
    "\n",
    "Below we want to demonstrate how conditional logic works in Nextflow workflows. We will extend the previous `count_words.nf` to have two boolean flags in our pipline `--reverse` (reverse the words in our final output) and `--count_letters` (if the flag is set, count the letters insteads of words in the final output)\n",
    "\n",
    "##### Add `reverse` function in the body of the workflow:\n",
    "```bash\n",
    "params.reverse = false\n",
    "\n",
    "workflow {\n",
    "  hello_result = null\n",
    "\n",
    "  if (params.reverse) {\n",
    "    hello_result = reverse(sayHello(params.name))\n",
    "  } else {\n",
    "    hello_result = sayHello(params.name)\n",
    "  }\n",
    "\n",
    "  count(hello_result) | view\n",
    "}\n",
    "```\n",
    "\n",
    "##### Add `count_letters` function in the function of `count`:\n",
    "```bash\n",
    "params.count_letters = false\n",
    "... \n",
    "\n",
    "process count {\n",
    "  input: \n",
    "    path(file_in)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   if (params.count_letters) {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -m ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -w ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "#### count_conditional.nf\n",
    "\n",
    "```bash\n",
    "!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "params.name = \"World\"\n",
    "params.count_letters = false\n",
    "params.reverse = false\n",
    "\n",
    "process sayHello {\n",
    "  input: \n",
    "    val name\n",
    "  output:\n",
    "    path \"hello.txt\"\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process count {\n",
    "  input: \n",
    "    path(file_in)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   if (params.count_letters) {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -m ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -w ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "\n",
    "process reverse {\n",
    "  input: \n",
    "    path(file_in)\n",
    "  output:\n",
    "    path \"reverse_hello.txt\"\n",
    "  \n",
    "  script:\n",
    "    \"\"\"\n",
    "    cat ${file_in} | rev > \"reverse_hello.txt\"\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  hello_result = null\n",
    "\n",
    "  if (params.reverse) {\n",
    "    hello_result = reverse(sayHello(params.name))\n",
    "  } else {\n",
    "    hello_result = sayHello(params.name)\n",
    "  }\n",
    "\n",
    "  count(hello_result) | view\n",
    "}\n",
    "```\n",
    "\n",
    "##### Example Outputs:\n",
    "```bash\n",
    "nextflow run workflows/count_samplesheet.nf --samplesheet samplesheet/samplesheet.csv\n",
    "```\n",
    "\n",
    "```bash\n",
    "nextflow run workflows/count_conditional.nf\n",
    "\n",
    "Hello World!\n",
    "2\n",
    "\n",
    "nextflow run workflows/count_conditional.nf --reverse\n",
    "!dlroW olleH\n",
    "2\n",
    "\n",
    "nextflow run workflows/count_conditional.nf --reverse --count_letters\n",
    "!dlroW olleH\n",
    "13\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd36dc",
   "metadata": {},
   "source": [
    "### Step 5. Parsing a Sample Sheet\n",
    "\n",
    "In real production Bioinformatics workflows, most of times we cannot just get by with specifying a list of fasta, raw or aligned reads. We also need to carry through all of the important metadata associated with a particular sample's sequencing data and provide their contexts to the analysis programs (e.g., Differential Gene Analysis to differentiate between `treatment` and `control` groups; temporal analysis to differentiate between the samples taken at different time points). \n",
    "\n",
    "We will demonstrate concept by making a toy counting workflow that takes a samplesheet of documents; and with a specificiation for each document to whether to count the text of the document by letters or words (determined by `count_letters` column).\n",
    "\n",
    "##### Make a simple samplesheet:\n",
    "Our toy sample sheet: \n",
    "\n",
    "```bash\n",
    "document,count_letters\n",
    "samplesheet/hello1.txt,F\n",
    "samplesheet/hello2.txt,T\n",
    "```\n",
    "\n",
    "##### Make a `documents_ch` as a channel of documents; parsed from the samplesheet above:\n",
    "\n",
    "Using `Channel.fromPath(params.samplesheet).splitCsv(header:true)`, We create a \"channel\" (or a \"stream\") of documents; where each entry is a tuple of both the document file itself and a boolean flag whether to count it by words or letters:\n",
    "`[ document, count_letters ]`.\n",
    "\n",
    "###### count_samplesheet.nf:\n",
    "```bash\n",
    "...\n",
    "// convert each row in the samplesheet into a tuple of File object and a Boolean of count_letter\n",
    "def get_document_info(LinkedHashMap sample) {\n",
    "    document  = sample.document ? file(sample.document, checkIfExists: true) : null\n",
    "    count_letters = (sample.count_letters == \"T\") || (sample.count_letters == \"true\") ? true : false   \n",
    "\n",
    "    return [ document, count_letters ]\n",
    "}\n",
    "...\n",
    "workflow {\n",
    "     // create a Channel of documents to count either by word or by letter, from the samplesheet\n",
    "     Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_document_info(it) }.set { documents_ch }\n",
    "            \n",
    "     // launch a sub-workflow of COUNT_DOCUMENT on each document\n",
    "     COUNT_DOCUMENT(documents_ch)\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "##### Define the sub-workflow `COUNT_DOCUMENT` which will process each individual document and to count appropriately by its metadata flag:\n",
    "\n",
    "Instead of refering to a global variable `params.count_letters` as we did previously. \n",
    "\n",
    "We refer to the current metadata we are processing: `tuple file(file_in), val(count_letters)`; which is: `[(samplesheet/hello1.txt,F), (samplesheet/hello2.txt,T)]`. And for `T`, we will count by letters; and for `F`, by words.\n",
    "\n",
    "```bash\n",
    "process count_document {\n",
    "  input: \n",
    "    tuple file(file_in), val(count_letters)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   if (count_letters) {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -m ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -w ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "\n",
    "workflow COUNT_DOCUMENT {\n",
    "    take:\n",
    "        input_ch\n",
    "\n",
    "    main:\n",
    "        count_document(input_ch) | view\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "##### count_samplesheet.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "process count_document {\n",
    "  input: \n",
    "    tuple file(file_in), val(count_letters)\n",
    "  output:\n",
    "    stdout\n",
    "  \n",
    "  script:\n",
    "   if (count_letters) {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -m ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    cat ${file_in}\n",
    "    wc -w ${file_in} | awk '{print \\$1}'\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "\n",
    "// convert each row in the samplesheet into a tuple of File object and a Boolean of count_letter\n",
    "def get_document_info(LinkedHashMap sample) {\n",
    "    document  = sample.document ? file(sample.document, checkIfExists: true) : null\n",
    "    count_letters = (sample.count_letters == \"T\") || (sample.count_letters == \"true\") ? true : false   \n",
    "\n",
    "    return [ document, count_letters ]\n",
    "}\n",
    "\n",
    "workflow COUNT_DOCUMENT {\n",
    "    take:\n",
    "        input_ch\n",
    "\n",
    "    main:\n",
    "        count_document(input_ch) | view\n",
    "}\n",
    "\n",
    "workflow {\n",
    "     // create a Channel of documents to count either by word or by letter, from the samplesheet\n",
    "     Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_document_info(it) }.set { documents_ch }\n",
    "    \n",
    "    // launch a sub-workflow of COUNT_DOCUMENT on each document\n",
    "     COUNT_DOCUMENT(documents_ch)\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "##### Example Run:\n",
    "```bash\n",
    "nextflow run workflows/count_samplesheet.nf --samplesheet samplesheet/samplesheet.csv\n",
    "```\n",
    "\n",
    "##### Example Outputs:\n",
    "\n",
    "```bash\n",
    "N E X T F L O W  ~  version 23.04.2\n",
    "Launching `workflows/count_samplesheet.nf` [dreamy_fourier] DSL2 - revision: c7971cd787\n",
    "executor >  local (2)\n",
    "[3f/9df991] process > COUNT_DOCUMENT:count_document (1) [100%] 2 of 2 ✔\n",
    "Count Me (using Letters)24\n",
    "\n",
    "Count Me (using Words)4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3456c3c",
   "metadata": {},
   "source": [
    "### Step 6. Asynchronous and synchronous job execution (or scatter-gather pattern)\n",
    "\n",
    "Scatter-gather pattern is an important pattern in pipelining. The concept is we want to split (\"scatter\") out entries on a fasta file or rows in a samplesheet for parallel execution and on any downstream jobs asynchronously (on as many nodes as possible on `OSCAR`). \n",
    "\n",
    "Then in the end of pipeline, we would like to aggregate (\"gather\") these results; for examples, \n",
    " - In RNASeq, to sum up or contrast the gene expressions of the `wild-type` replicates against the `treatment` replicates\n",
    " - In metagenomics workflows, to summarize the overall distribution or diversity of all the samples in a data-set.\n",
    " \n",
    " \n",
    "In our toy example of document counting, we would like to demonstrate this concept by simply gathering all of the individual counts (whether by letter or by words) and summing all of them together.\n",
    "\n",
    "\n",
    "###### Add a variable `count_result` which is an synchronous collection/`collect()` of all the scattered or asynchronous runs of `COUNT_DOCUMENT`:\n",
    "\n",
    "By defining a variable to hold onto `COUNT_DOCUMENT(documents_ch).collect()`, we are essentially assigning `count_results` as the gather-step's result placeholder. In another words, we will wait for all `COUNT_DOCUMENTS` to finish, and all individual document counts gathered into `count_results` and then run the final gather step of `sum_all_results(count_results)`. \n",
    "\n",
    "###### count_and_summarize.nf:\n",
    "```bash\n",
    "process sum_all_results {\n",
    "  input: \n",
    "    path (files)\n",
    "  output:\n",
    "    stdout\n",
    "\n",
    "  script:\n",
    "    \"\"\"\n",
    "    cat ${files} > combined.txt\n",
    "    awk '{ sum += \\$1 } END { print sum }' combined.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "    ... \n",
    "    // gather all of the results of each document counting sub-workflow\n",
    "    count_results = COUNT_DOCUMENT(documents_ch).collect()\n",
    "    \n",
    "    // and feed this collection of results into a summing function\n",
    "    sum_all_results(count_results) | view\n",
    "}\n",
    "```\n",
    "\n",
    "###### count_and_summarize.nf (full): \n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "process count_document {\n",
    "  input: \n",
    "    tuple file(file_in), val(count_letters)\n",
    "  output:\n",
    "    file \"${file_in.baseName}_count.txt\"\n",
    "  \n",
    "  script:\n",
    "   if (count_letters) {\n",
    "    \"\"\"\n",
    "    wc -m ${file_in} | awk '{print \\$1}' > ${file_in.baseName}_count.txt\n",
    "    \"\"\"\n",
    "   } else {\n",
    "    \"\"\"\n",
    "    wc -w ${file_in} | awk '{print \\$1}' > ${file_in.baseName}_count.txt\n",
    "    \"\"\"\n",
    "   }\n",
    "}\n",
    "\n",
    "process sum_all_results {\n",
    "  input: \n",
    "    path (files)\n",
    "  output:\n",
    "    stdout\n",
    "\n",
    "  script:\n",
    "    \"\"\"\n",
    "    cat ${files} > combined.txt\n",
    "    awk '{ sum += \\$1 } END { print sum }' combined.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "// convert each row in the samplesheet into a tuple of File object and a Boolean of count_letter\n",
    "def get_document_info(LinkedHashMap sample) {\n",
    "    document  = sample.document ? file(sample.document, checkIfExists: true) : null\n",
    "    count_letters = (sample.count_letters == \"T\") || (sample.count_letters == \"true\") ? true : false   \n",
    "\n",
    "    return [ document, count_letters ]\n",
    "}\n",
    "\n",
    "workflow COUNT_DOCUMENT {\n",
    "    take:\n",
    "        input_ch\n",
    "\n",
    "    main:\n",
    "        word_or_letter_counts = count_document(input_ch)\n",
    "    \n",
    "    emit:\n",
    "        word_or_letter_counts\n",
    "}\n",
    "\n",
    "workflow {\n",
    "     // create a Channel of documents to count either by word or by letter, from the samplesheet\n",
    "     Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_document_info(it) }.set { documents_ch }\n",
    "    \n",
    "    // launch a sub-workflow of COUNT_DOCUMENT on each document\n",
    "    count_results = COUNT_DOCUMENT(documents_ch).collect()\n",
    "    sum_all_results(count_results) | view\n",
    "}\n",
    "```\n",
    "\n",
    "##### Example Run:\n",
    "```bash\n",
    "nextflow run workflows/count_and_summarize.nf --samplesheet samplesheet/samplesheet.csv\n",
    "```\n",
    "\n",
    "##### Example Outputs:\n",
    "```bash\n",
    "N E X T F L O W  ~  version 23.04.2\n",
    "Launching `workflows/count_and_summarize.nf` [zen_baekeland] DSL2 - revision: 478f5774ff\n",
    "executor >  local (3)\n",
    "[12/8215ea] process > COUNT_DOCUMENT:count_document (1) [100%] 2 of 2 ✔\n",
    "[ff/fbd4b5] process > sum_all_results                   [100%] 1 of 1 ✔\n",
    "28\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2910c",
   "metadata": {},
   "source": [
    "#### Step 7. Putting it all together in a real-world RNASeq example\n",
    "\n",
    "We have learned all of the essential patterns involved in creating pipelines; and now we can begin to put all of them together in a simple RNASeq pipeline. Below is the code for a full RNASeq pipeline, don't worry too much about understanding completely all of the steps. \n",
    "\n",
    "Our purpose here is to refresh/review the pipeline patterns we just learned applied to a real world example. Each application of the patterns will be called out following the code. \n",
    "\n",
    "###### rnaseq_simple.nf:\n",
    "\n",
    "```bash\n",
    "workflow PROCESS_SAMPLE {\n",
    "    take:\n",
    "        input_ch\n",
    "        reference_genome\n",
    "\n",
    "    main:\n",
    "        fastqc(input_ch)\n",
    "\n",
    "        trimmed_reads = trimmomatic(input_ch)\n",
    "\n",
    "        fastqcs = fastqc2(trimmed_reads).collect()\n",
    "        fastqc_screens = fastq_screen(trimmed_reads).collect()\n",
    "\n",
    "        aligned_bams = star(trimmed_reads, reference_genome)\n",
    "        marked_duplicate_bams = mark_duplicate(aligned_bams)\n",
    "\n",
    "        qualimaps = qualimap(marked_duplicate_bams).collect()\n",
    "\n",
    "        htseq_counts = htseq_count(marked_duplicate_bams).collect()\n",
    "        feature_counts = feature_count(marked_duplicate_bams)\n",
    "\n",
    "        multiqc(fastqcs, fastqc_screens, qualimaps, htseq_counts)\n",
    "}\n",
    "\n",
    "// Function to resolve files\n",
    "def get_sample_info(LinkedHashMap sample) {\n",
    "    read1  = sample.read1 ? file(sample.read1, checkIfExists: true) : null\n",
    "    read2 = sample.read2 ? file(sample.read2, checkIfExists: true) : null\n",
    "\n",
    "    return [ sample.sample_id, read1, read2 ]\n",
    "}\n",
    "\n",
    "workflow {\n",
    "     reference_genome = params.reference_genome\n",
    "\n",
    "     if (params.reference_genome_fasta != \"\") {\n",
    "        reference_genome = build_star_index()\n",
    "     }\n",
    "\n",
    "     Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_sample_info(it) }.set { samples_ch }\n",
    "\n",
    "     PROCESS_SAMPLE (samples_ch, reference_genome)\n",
    "}\n",
    "```\n",
    "\n",
    "###### Pattern 1: Conditional Logic\n",
    "\n",
    "If `--reference_genome_fasta` is specified, we will build an index for this reference genome (so that our raw reads could be aligned against this index more efficiently)\n",
    "\n",
    "```bash\n",
    "reference_genome = params.reference_genome\n",
    "\n",
    "if (params.reference_genome_fasta != \"\") {\n",
    "    reference_genome = build_star_index()\n",
    "}\n",
    "\n",
    "... # use reference_genome everywhere\n",
    "```\n",
    "\n",
    "###### Pattern 2: Parsing the Samplesheet\n",
    "We parse a samplesheet of RNASeq samples which will be `(sample_id, read1, read2)` and carry this metadata throughout the rest of the workflow.\n",
    "\n",
    "```bash\n",
    "\n",
    "// Function to resolve files\n",
    "def get_sample_info(LinkedHashMap sample) {\n",
    "    read1  = sample.read1 ? file(sample.read1, checkIfExists: true) : null\n",
    "    read2 = sample.read2 ? file(sample.read2, checkIfExists: true) : null\n",
    "\n",
    "    return [ sample.sample_id, read1, read2 ]\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "Channel.fromPath(params.samplesheet).splitCsv(header:true)\n",
    "            .map { get_sample_info(it) }.set { samples_ch }\n",
    "PROCESS_SAMPLE (samples_ch, reference_genome)\n",
    "```\n",
    "\n",
    "###### Pattern 3: Scatter-Gather using `collect()`\n",
    "We will process asynchronously all of the tasks for each RNASeq sample such as:\n",
    "- QCing the read `fastqc`\n",
    "- Screening the read for contamination/sequencing artifacts `fastq_screen`\n",
    "- Aligning the reads and marking duplicates on the aligned reads `star` aligner and `mark_duplicate` tool\n",
    "- Count the features (individual gene expression) on the algined reads `htseq`\n",
    "\n",
    "Then finally, we gather all of these results into an awesome data visualization/summarization tool `multiqc`. \n",
    "\n",
    "```bash\n",
    "        fastqcs = fastqc2(trimmed_reads).collect()\n",
    "        fastqc_screens = fastq_screen(trimmed_reads).collect()\n",
    "\n",
    "        ...\n",
    "\n",
    "        qualimaps = qualimap(marked_duplicate_bams).collect()\n",
    "        htseq_counts = htseq_count(marked_duplicate_bams).collect()\n",
    "\n",
    "        multiqc(fastqcs, fastqc_screens, qualimaps, htseq_counts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa5dc2",
   "metadata": {},
   "source": [
    "### Step 9: How to Import Secrets\n",
    "\n",
    "\n",
    "Put a token or secret value as a nextflow `secrets`: \n",
    "```commandline\n",
    "nextflow secrets set dscov_secret_location RI\n",
    "```\n",
    "\n",
    "###### secret.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    secret 'dscov_secret_location'\n",
    "  output:\n",
    "    stdout\n",
    "  script:\n",
    "    \"\"\"\n",
    "    curl https://api.weather.gov/alerts/active?area=\\$dscov_secret_location\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  sayHello() | view\n",
    "}\n",
    "```\n",
    "\n",
    "##### Example Run:\n",
    "```bash\n",
    "nextflow run workflows/secret.nf\n",
    "```\n",
    "\n",
    "##### Example output:\n",
    "![](./img/secret_output.png)\n",
    "\n",
    "##### Workflow logs (will not reveal the secret value/token/key...):\n",
    "```bash\n",
    "#!/bin/bash -ue\n",
    "curl https://api.weather.gov/alerts/active?area=$dscov_secret_location\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Step 10: Dynamically Size VMs Memory:\n",
    "\n",
    "##### dynamic_memory.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    tuple path(\"hello.txt\"), stdout\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    echo \\$(( \\$( stat -c '%s' hello.txt ) * 3 )) ### get the size of the output file and multiply it by 3\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process countWords {\n",
    "  input:\n",
    "    tuple path(file_in), val(file_size)\n",
    "  output:\n",
    "    path(\"count_words.txt\")\n",
    "  memory \"${file_size}\"\n",
    "\n",
    "  publishDir \"${params.out_dir}/\", mode: 'copy'\n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "   wc -w ${file_in} | awk '{print \\$1}' > count_words.txt\n",
    "   \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name))\n",
    "}\n",
    "```\n",
    "\n",
    "##### Example Run:\n",
    "```bash\n",
    "nextflow run workflows/dynamic_memory.nf\n",
    "```\n",
    "\n",
    "##### Slurm Wrapper Command:\n",
    "```bash \n",
    "cat .command.run\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -J nf-sayHello\n",
    "#SBATCH -o /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.log\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --signal B:USR2@30\n",
    "#SBATCH -c 2\n",
    "#SBATCH -t 02:00:00\n",
    "#SBATCH --mem 0M\n",
    "#SBATCH -p batch\n",
    "NXF_CHDIR=/gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66\n",
    "# NEXTFLOW TASK: sayHello\n",
    "...\n",
    "/bin/bash -ue /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.sh &\n",
    "...\n",
    "```\n",
    "\n",
    "### Step 12: Using Singularity\n",
    "```\n",
    "process countWords {\n",
    "  container 'cowmoo/rnaseq_pipeline:latest'\n",
    "  containerOptions '--bind /oscar/data/shared/databases/refchef_refs:/oscar/data/shared/databases/refchef_refs'\n",
    "\n",
    "  input:\n",
    "    tuple path(file_in), val(file_size)\n",
    "  output:\n",
    "    path(\"count_words.txt\")\n",
    "  memory \"${file_size}\"\n",
    "\n",
    "  publishDir \"${params.out_dir}/\", mode: 'copy'\n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "   wc -w ${file_in} | awk '{print \\$1}' > count_words.txt\n",
    "   \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name))\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "##### Slurm Wrapper Command:\n",
    "```bash \n",
    "cat .command.run\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -J nf-sayHello\n",
    "#SBATCH -o /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.log\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --signal B:USR2@30\n",
    "#SBATCH -c 2\n",
    "#SBATCH -t 02:00:00\n",
    "#SBATCH --mem 512M\n",
    "#SBATCH -p batch\n",
    "NXF_CHDIR=/gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66\n",
    "# NEXTFLOW TASK: sayHello\n",
    "...\n",
    "/bin/bash -ue \n",
    "singularity exec -B /oscar/data/cbc/ -B \"$PWD\" --bind /oscar/data/shared/databases/refchef_refs:/oscar/data/shared/databases/refchef_refs -/oscar/data/cbc/sanghyun_lee/work/singularity/cowmoo-rnaseq_pipeline-latest.img /bin/bash -c \"cd $PWD; /bin/bash -ue /oscar/data/cbc/sanghyun_lee/work//52/992fb76ea549e001e25b749b615f66/.command.sh\"\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "### Step 13: Coming Soon Curated Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60334a",
   "metadata": {},
   "source": [
    "![](./img/nfcore_screenshot.png)\n",
    "\n",
    "https://nf-co.re/pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
