{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4601bc",
   "metadata": {},
   "source": [
    "<h1><center>Basic Bioinformatics Workflows on OSCAR: Snakemake and Nextflow</center></h1>\n",
    "<p><center>Instructors: Ashok Ragavendran and Jordan Lawson</center>\n",
    " <center>Center for Computation and Visualization</center>\n",
    " <center>Center for Computational Biology of Human Disease - Computational Biology Core</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfe5ec",
   "metadata": {},
   "source": [
    "Resources for help @brown <br> \n",
    "\n",
    "COBRE CBHD Computational Biology Core\n",
    "- Office hours\n",
    "- https://cbc.brown.edu\n",
    "- slack channel on ccv-share\n",
    "- cbc-help@brown.edu <br>\n",
    "\n",
    "Center for Computation and Visualization\n",
    "- Office hours\n",
    "- https://ccv.brown.edu\n",
    "- ccv-share slack channel\n",
    "- https://docs.ccv.brown.edu\n",
    "- support@ccv.brown.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2fc35",
   "metadata": {},
   "source": [
    "## What is Snakemake and Nextflow?  \n",
    "\n",
    "Snakemake and Nextflow are workflow management tools that allow users to easily write data-intensive computational **pipelines**. These pipelines, or workflows as they are also called, have the following key features:\n",
    "\n",
    "- Sequential processing of files\n",
    "- Usually requires more than one tool\n",
    "- Multiple programming languages\n",
    "- Most times each sample is processed individually\n",
    "- Compute resource intensive\n",
    "  - Alignment could take 16 cpus, 60 Gb RAM, 4-24 hours, 30Gb of disk space per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6649f1",
   "metadata": {},
   "source": [
    "## Why do we care about these pipelines? \n",
    "\n",
    "### Reason 1: Reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06abc40",
   "metadata": {},
   "source": [
    "<img src=\"./img/reproduce.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb742d3",
   "metadata": {},
   "source": [
    "The journal Nature published a survey that found that more than 70% of researchers have tried and failed to reproduce another scientist's experiments. This trend is hugely problematic because we then can't trust the findings from many studies enough to use them to make data-driven decisions. In short, we need tools and standards that help address the reproducibility crisis in science! \n",
    "\n",
    "Pipelines created with Snakemake and Nextflow incorporate version-control and state-of-the-art software tools, known as containers, to manage all software dependencies and create stable environments that can be trusted to reproduce analyses reliably and accurately. \n",
    "\n",
    "***Reproducibility is important for producing trustworthy research!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bbec0",
   "metadata": {},
   "source": [
    "### Reason 2: Portability\n",
    "\n",
    "#### What if we need to perform analyses with more resources?\n",
    "\n",
    "This type of scenario would require us to move our analyses to a different environment, for example, a High Performance Computing (HPC) cluster environment. \n",
    "\n",
    "An important feature of Snakemake and Nextflow workflow management tools is that they enable users to easily scale any pipeline written on a personal computer to then run on an HPC cluster such as OSCAR, the HPC cluster we use at Brown University. So now we can run our pipelines using high performance resources without having to change workflow definitions or hard-code a pipeline to a specific setup. As a result, **the code stays constant** across varying infrastructures, thereby allowing portability, easy collaboration, and avoiding lock-in. \n",
    "\n",
    "***In short, we can easily move our multi-step analyses (i.e., pipelines) to any place we need them!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d46b76",
   "metadata": {},
   "source": [
    "## So Let's See How All This Works! \n",
    "\n",
    "### Our Starting Point\n",
    "\n",
    "Say we have samples from reduced representation bisulfite sequencing (RRBS data) that we need to process on OSCAR by performing the following set of actions: \n",
    "\n",
    "<img src=\"./img/workflow.png\" width=\"700\"/>\n",
    "\n",
    "<h2><center>What do you do??</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbe0a7",
   "metadata": {},
   "source": [
    "## A Naive Approach\n",
    "\n",
    "One solution would be to write a bunch of shell scripts that use various software tools to process the data in the ways we need. \n",
    "\n",
    "For example, if we need to run fastqc, trim galore, and then an alignment, we could just write a shell script for each step - so a total of 4 shell scripts in this case - where we have various inputs and outputs. This would look something as follows: \n",
    "\n",
    "**Script 1: Fastqc**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J rrbs_fastqc\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/Sequencing_Files/*fastq.gz`\n",
    "do\n",
    "align_dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs\" \n",
    "fastqc -o ${align_dir}/fastqc $sample\n",
    "done\n",
    "```\n",
    "\n",
    "**Script 2: Trimming** \n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J trimmomatic_update\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trim_galore/*_trimmed.fq.gz`\n",
    "do\n",
    "    dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic\"\n",
    "    base=$(basename $sample \"_trimmed.fq.gz\")\n",
    "    trimmomatic SE  -threads 8 -trimlog ${dir}/${base}_SE.log $sample ${dir}/${base}_tr.fq.gz ILLUMINACLIP:/gpfs/data/cbc/cbc_conda_v1/envs/cbc_conda/opt/trimmomatic-0.36/adapters/TruSeq3-SE.fa:2:30:5:6:true SLIDINGWINDOW:4:20 MINLEN:35\n",
    "done \n",
    "```\n",
    "\n",
    "\n",
    "**Script 3: Fastqc on trimmed reads**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -n 8\n",
    "#SBATCH -J retrim_fastqc_update\n",
    "#SBATCH --mem=16GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "for sample in `ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic/*_tr.fq.gz`\n",
    "do\n",
    "trim_qc_dir=\"/gpfs/data/cbc/fedulov_alexey/porcine_rrbs\"\n",
    "fastqc -o ${trim_qc_dir}/trimmomatic_qc $sample\n",
    "done\n",
    "```\n",
    "\n",
    "**Script 4: Alignment**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 16\n",
    "#SBATCH -J bismark_align_update_redo\n",
    "#SBATCH --mem=160GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "#SBATCH --array=1-18\n",
    "#SBATCH -e /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/logs/bismark_align_%a_%A_%j.err\n",
    "#SBATCH -o /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/logs/bismark_align_%a_%A_%j.out\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate fedulov_rrbs\n",
    "input=($(ls /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/trimmomatic/*_tr.fq.gz)) # using the round brackets indicates that this is a bash array\n",
    "bismark -o /gpfs/data/cbc/fedulov_alexey/porcine_rrbs/alignments --bowtie2 --genome /gpfs/data/shared/databases/refchef_refs/S_scrofa/primary/bismark_index --un --pbat ${input[$((SLURM_ARRAY_TASK_ID -1))]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5d964",
   "metadata": {},
   "source": [
    "## Problems with the Naive Approach \n",
    "\n",
    "Using multiple shell scripts to create a makeshift pipeline will work, but it is **inefficient**, can **get complicated fast**, and there are a few **challenges you have to manage**, such as: \n",
    "\n",
    "* Making sure you have the appropriate software and all dependencies for each step in the analysis - this can be a lot to stay on top of if you have a pipeline with a lot of steps! (imagine a 10 step process)\n",
    "* **Portability!** Building and running on different machines is much more work\n",
    "* Specifying where your output will go \n",
    "* Calling in the appropriate input (which is often the output from a previous step) \n",
    "* Handling where log files go \n",
    "* More labor intensive - we have to stay on top of jobs and monitor when each step finishes and then run next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb9e7b",
   "metadata": {},
   "source": [
    "## A Smarter Approach: Using Workflow Managers! \n",
    "\n",
    "The solution for processing your data in a much more efficient manner that handles the aforementioned issues is workflow management tools, such as Snakemake and Nextflow. Let's now learn how to use Snakemake and Nextflow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7259fc",
   "metadata": {},
   "source": [
    "## Tutorial: Using workflow management tools on OSCAR \n",
    "\n",
    "Workflow management tools are software that allow you to write more efficient, portable computational pipelines. As a result, you are able to optimize your workflows while maintaining reproducibility and rigor. Note that there are many workflow management tools available to researchers, but the two tools we will focus on learning about today are **Snakemake** and **Nextflow**. Let's first start with Nextflow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09894a3",
   "metadata": {},
   "source": [
    "### Step 1: The Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdc919",
   "metadata": {},
   "source": [
    "Let's first discuss setting up our environment on OSCAR so that we can get Snakemake up and running. \n",
    "\n",
    "**At this point, I am going to open my terminal on Open OnDemand (OOD) so that I can walk you through and show you how each of these steps and files below look. Feel free to open your terminal as well and follow along. To do so, you can go to OOD at https://ood.ccv.brown.edu and under the Clusters tab at the top select the >_OSCAR Shell Access option. All files used today can be found on GitHub in the folder at: https://github.com/compbiocore/workflows_on_OSCAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32339e10",
   "metadata": {},
   "source": [
    "#### Step 1a: Set up Nextflow Configuration Script using `compbiocore/workflows_on_OSCAR`:\n",
    "\n",
    "```bash\n",
    "[pcao5@node1322 ~]$ cd ~/\n",
    "[pcao5@node1322 ~]$ git clone https://github.com/compbiocore/workflows_on_OSCAR.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944acb9",
   "metadata": {},
   "source": [
    "#### Step 1b: Install `compbiocore/workflows_on_OSCAR` package:\n",
    "\n",
    "```bash\n",
    "bash ~/workflows_on_OSCAR/install_me/install.sh && source ~/.bashrc\n",
    "```\n",
    "\n",
    "\n",
    "For the 1st installation prompt, input `NextFlow`:\n",
    "\n",
    "```bash\n",
    "Welcome to a program designed to help you easily set up and run workflow management systems on OSCAR!\n",
    "\n",
    "Please type which software you wish to use: Nextflow or Snakemake? Nextflow\n",
    "```\n",
    "\n",
    "For the 2nd installation prompt, input your GitHub username (e.g., `paulcao-brown`):\n",
    "\n",
    "```bash\n",
    "Nextflow software detected, initializing configuration...\n",
    "What is your GitHub user name? paulcao-brown\n",
    "What is your GitHub token (we will keep this secret) - [Hit Enter when Done]?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d19eb",
   "metadata": {},
   "source": [
    "#### Step 1c: Create a new GitHub Token and enter it:\n",
    "<img src=\"https://i.imgur.com/GBGDQhY.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109d512",
   "metadata": {},
   "source": [
    "#### Step 1d: Complete the Installation \n",
    "\n",
    "\n",
    "```bash\n",
    "Currently the Nextflow default for HPC resources is: memory = 5.GB time = 2.h cpus = 2 \n",
    "Do you want to change these default resources for your Nextflow pipeline [Yes or No]? No\n",
    "Keeping defaults!\n",
    "\n",
    "OUTPUT MESSAGE:\n",
    "\n",
    "                ******************************************************************\n",
    "                 NEXTFLOW is now set up and configured and ready to run on OSCAR!\n",
    "                ******************************************************************\n",
    "                \n",
    "\n",
    "Your default resources for Nextflow are: memory = 5.GB time = 2.h cpus = 2 \n",
    "\n",
    "\n",
    "                To further customize your pipeline for efficiency, you can enter the following \n",
    "                label '<LabelName>' options right within processes in your Nextflow .nf script:\n",
    "                1. label 'OSCAR_small_job' (comes with memory = 4 GB, time = 1 hour, cpus = 1)\n",
    "                2. label 'OSCAR_medium_job' (comes with memory = 8 GB, time = 16 hours, cpus = 2)\n",
    "                3. label 'OSCAR_large_job' (comes with memory = 16 GB, time = 24 hours, cpus = 2)\n",
    "                \n",
    "\n",
    "README:\n",
    "\n",
    "Please see https://github.com/compbiocore/workflows_on_OSCAR for further details on how to add the above label options to your workflow.\n",
    "\n",
    "Note the setup is designed such that pipelines downloaded from nf-core with their own resource specs within the .nf script will override your defaults.\n",
    "\n",
    "To run Nextflow commands, you must first type and run the nextflow_start command.\n",
    "\n",
    "To further learn how to easily run your Nextflow pipelines on OSCAR, use the Nextflow template shell script located in your ~/nextflow_setup directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2bd29",
   "metadata": {},
   "source": [
    "### Step 2: Run a 'Hello World' Example\n",
    "\n",
    "#### 2a. hello_world.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    stdout\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!'\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  sayHello(params.name) | view\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2b. Launch the Workflow:\n",
    "```bash\n",
    "nextflow run hello_world.nf\n",
    "```\n",
    "\n",
    "#### 2c. Workflow Output:\n",
    "![](https://i.imgur.com/8bNSCPv.png)\n",
    "\n",
    "#### 2d. Launch the Workflow (Custom Parameter):\n",
    "```bash\n",
    "nextflow run hello_world.nf --name \"Bleuno Bear\"\n",
    "```\n",
    "\n",
    "#### 2e. Workflow Output (Custom Parameter):\n",
    "![](https://i.imgur.com/YpjvNoe.png)\n",
    "\n",
    "#### 2f. Inspect Underneath the Hood the Command (e.g.,`52/992fb7`):\n",
    "```bash\n",
    "cd work/52/992fb76ea549e001e25b749b615f66/\n",
    "ls -la\n",
    "```\n",
    "\n",
    "##### Nextflow Generated Commands:\n",
    "\n",
    "![](https://i.imgur.com/JK1Ly1s.png)\n",
    "\n",
    "\n",
    "##### The Actual Command Run:\n",
    "```bash\n",
    "cat .command.sh\n",
    "\n",
    "#!/bin/bash -ue\n",
    "echo 'Hello Blueno Bear!'\n",
    "```\n",
    "\n",
    "##### Slurm Wrapper Command:\n",
    "```bash \n",
    "cat .command.run\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -J nf-sayHello\n",
    "#SBATCH -o /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.log\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --signal B:USR2@30\n",
    "#SBATCH -c 2\n",
    "#SBATCH -t 02:00:00\n",
    "#SBATCH --mem 5120M\n",
    "#SBATCH -p batch\n",
    "NXF_CHDIR=/gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66\n",
    "# NEXTFLOW TASK: sayHello\n",
    "...\n",
    "/bin/bash -ue /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66/.command.sh &\n",
    "...\n",
    "```\n",
    "\n",
    "##### Standard Out/Error:\n",
    "```bash \n",
    "cat .command.log\n",
    "## SLURM PROLOG ###############################################################\n",
    "##    Job ID : 10159219\n",
    "##  Job Name : nf-sayHello\n",
    "##  Nodelist : node1322\n",
    "##      CPUs : 4\n",
    "##  Mem/Node : 5120 MB\n",
    "## Directory : /gpfs/data/cbc/workflow_workshop/work/52/992fb76ea549e001e25b749b615f66\n",
    "##   Job Started : Wed Jun  7 10:34:20 EDT 2023\n",
    "###############################################################################\n",
    "\n",
    "Hello Blueno Bear!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5b85e",
   "metadata": {},
   "source": [
    "### Step 3: Run a 'Word Count' Example (2 Step Workflow) \n",
    "\n",
    "#### 3a. word_count.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    path \"hello.txt\"\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process countWords {\n",
    "  input:\n",
    "    path(file_in)\n",
    "  output:\n",
    "    stdout\n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "   cat ${file_in}\n",
    "   wc -w ${file_in} | awk '{print \\$1}'\n",
    "   \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name)) | view\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3b. Launch the Workflow:\n",
    "```bash\n",
    "nextflow run count_words.nf --name \"Blueno Bear\"\n",
    "```\n",
    "\n",
    "#### 3c. Workflow Output:\n",
    "![](https://i.imgur.com/649M0u8.png)\n",
    "\n",
    "\n",
    "In the next iteration, we want to save the word count to a text file; so we add to `word_count.nf` the following lines:\n",
    "- add this directive `publishDir \"${params.out_dir}/\", mode: 'copy'` to `countWords`\n",
    "- and change the `countWords` function to redirect output to `wc -w ${file_in} | awk '{print \\$1}' > count_words.txt`\n",
    "\n",
    "#### 3d. word_count_and_save.nf\n",
    "\n",
    "```bash\n",
    "!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "params.name = \"World\"\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "    val name\n",
    "  output:\n",
    "    path \"hello.txt\"\n",
    "  script:\n",
    "    \"\"\"\n",
    "    echo 'Hello ${name}!' > hello.txt\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process countWords {\n",
    "  input:\n",
    "    path(file_in)\n",
    "  output:\n",
    "    path(\"count_words.txt\")\n",
    "\n",
    "  publishDir \"${params.out_dir}/\", mode: 'copy'\n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "   wc -w ${file_in} | awk '{print \\$1}' > count_words.txt\n",
    "   \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "  countWords(sayHello(params.name))\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3e. Launch the Workflow\n",
    "```bash\n",
    "nextflow run word_count_and_save.nf --name \"Blueno Bear\" --out_dir words_out\n",
    "```\n",
    "\n",
    "##### Workflow Output (Custom Parameter):\n",
    "![](https://i.imgur.com/nrrpAGV.png)\n",
    "\n",
    "##### Inspect the output directory `words_out`:\n",
    "```bash\n",
    "cat words_out/count_words.txt\n",
    "\n",
    "3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2910c",
   "metadata": {},
   "source": [
    "### Step 4: Run a reduced representation bisulfite sequencing (RRBS) Workflow\n",
    "\n",
    "#### 4a. rrbs_sequencing_single.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "params.read = \"/gpfs/data/cbc/workflow_workshop/sample1.fq.gz\"\n",
    "\n",
    "process trimmomatic {\n",
    "  \n",
    "  input: \n",
    "    path(read)\n",
    "\n",
    "  publishDir \"${params.out_dir}/trimmed_reads\", mode: 'copy'\n",
    "  \n",
    "  output:\n",
    "    path(\"*_tr.fq.gz\")\n",
    "  \n",
    "  script:\n",
    "    \"\"\"\n",
    "      module load trimmomatic/0.39\n",
    "      TrimmomaticSE ${read} ${read.getBaseName()}_tr.fq.gz ILLUMINACLIP:/gpfs/data/cbc/cbc_conda_v1/envs/cbc_conda/opt/trimmomatic-0.36/adapters/TruSeq3-SE.fa:2:30:5:6:true SLIDINGWINDOW:10:25 MINLEN:50\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process fastqc {\n",
    "  input:\n",
    "    path(read)\n",
    "\n",
    "  publishDir \"${params.out_dir}/fastqc_original_reads\", mode: 'copy'\n",
    "\n",
    "  output:\n",
    "    path \"*\"\n",
    "\n",
    "  script:\n",
    "    \"\"\"\n",
    "      module load fastqc/0.11.5\n",
    "      fastqc ${read}\n",
    "    \"\"\"\n",
    "   \n",
    "}\n",
    "\n",
    "process fastqc_trimmed {\n",
    "\n",
    "  input:\n",
    "    path(trimmed_read)\n",
    "\n",
    "  publishDir \"${params.out_dir}/fastqc_trimmed_reads\", mode: 'copy'\n",
    "\n",
    "  output:\n",
    "    path \"*\"\n",
    "\n",
    "  script:\n",
    "    \"\"\"\n",
    "      module load fastqc/0.11.5\n",
    "      fastqc ${trimmed_read}\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "process alignment {\n",
    "  \n",
    "  input:\n",
    "    path(trimmed_read)\n",
    "\n",
    "  publishDir \"${params.out_dir}/alignments\", mode: 'copy'\n",
    "\n",
    "  output:\n",
    "   path \"*.sam.gz\"\n",
    "   path \"*_report.txt\"\n",
    "   path \"*_unmapped_reads.fq.gz\"\n",
    "   \n",
    "\n",
    "  script:\n",
    "   \"\"\"\n",
    "    module load bismark/0.20.0\n",
    "    module load bowtie2/2.4.2\n",
    "    bismark -o `pwd` --bowtie2 --genome /gpfs/data/shared/databases/refchef_refs/S_scrofa/primary/bismark_index --un --pbat ${trimmed_read}\n",
    "   \"\"\"  \n",
    "\n",
    "} \n",
    "\n",
    "workflow\n",
    "{\n",
    "  fastqc(file(params.read))\n",
    "  trimmed_result = trimmomatic(file(params.read))\n",
    "  fastqc_trimmed(trimmed_result)\n",
    "  alignment(trimmed_result)\n",
    "}\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de43105",
   "metadata": {},
   "source": [
    "#### 4b. Launch the Workflow:\n",
    "```bash\n",
    "nextflow run rrbs_sequencing_single.nf --read /gpfs/data/cbc/workflow_workshop/sample1.fq.gz --out_dir single_read_out\n",
    "```\n",
    "\n",
    "#### 4c. Workflow Output:\n",
    "![](https://i.imgur.com/hhDbWAP.png)\n",
    "\n",
    "#### 4d. Inspect Workflow Outputs: \n",
    "```bash\n",
    "ls -R single_read_out\n",
    "\n",
    "\n",
    "single_read_out:\n",
    "alignments  fastqc_original_reads  fastqc_trimmed_reads  report.html  timeline.html  trimmed_reads\n",
    "\n",
    "single_read_out/alignments:\n",
    "sample1.fq_tr.fq.gz_unmapped_reads.fq.gz  sample1.fq_tr_bismark_bt2.sam.gz  sample1.fq_tr_bismark_bt2_SE_report.txt\n",
    "\n",
    "single_read_out/fastqc_original_reads:\n",
    "sample1_fastqc.html  sample1_fastqc.zip\n",
    "\n",
    "single_read_out/fastqc_trimmed_reads:\n",
    "sample1.fq_tr_fastqc.html  sample1.fq_tr_fastqc.zip\n",
    "\n",
    "single_read_out/trimmed_reads:\n",
    "sample1.fq_tr.fq.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935833c2",
   "metadata": {},
   "source": [
    "### Step 5: Run a reduced representation bisulfite sequencing (RRBS) Workflow [Using a Input Directory]\n",
    "\n",
    "#### 5a. rrbs_sequencing.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "...\n",
    "\n",
    "workflow\n",
    "{\n",
    "  reads = Channel.fromPath(params.read_dir + \"/*.fq.gz\", checkIfExists: true)\n",
    "  fastqc(reads)\n",
    "  trimmed_result = trimmomatic(reads)\n",
    "  fastqc_trimmed(trimmed_result)\n",
    "  alignment(trimmed_result)\n",
    "}\n",
    "```\n",
    "\n",
    "#### 5b. Launch Workflow and Inspect Workflow Inputs:\n",
    "\n",
    "##### Launch Workflow\n",
    "```bash\n",
    "nextflow run rrbs_sequencing.nf --read_dir /gpfs/data/cbc/workflow_workshop/input_dir --out_dir multiple_read_out\n",
    "```\n",
    "\n",
    "##### Inspect the Input Directory:\n",
    "```bash\n",
    "ls -R /gpfs/data/cbc/workflow_workshop/input_dir\n",
    "\n",
    "/gpfs/data/cbc/workflow_workshop/input_dir:\n",
    "sample1.fq.gz  sample2.fq.gz\n",
    "```\n",
    "\n",
    "\n",
    "##### Inspect the Output\n",
    "```bash\n",
    "ls -R multiple_read_out \n",
    "\n",
    "multiple_read_out/:\n",
    "alignments  fastqc_original_reads  fastqc_trimmed_reads  report.html  timeline.html  trimmed_reads\n",
    "\n",
    "multiple_read_out/alignments:\n",
    "sample1.fq_tr.fq.gz_unmapped_reads.fq.gz  sample1.fq_tr_bismark_bt2_SE_report.txt   sample2.fq_tr_bismark_bt2.sam.gz\n",
    "sample1.fq_tr_bismark_bt2.sam.gz          sample2.fq_tr.fq.gz_unmapped_reads.fq.gz  sample2.fq_tr_bismark_bt2_SE_report.txt\n",
    "\n",
    "multiple_read_out/fastqc_original_reads:\n",
    "sample1_fastqc.html  sample1_fastqc.zip  sample2_fastqc.html  sample2_fastqc.zip\n",
    "\n",
    "multiple_read_out/fastqc_trimmed_reads:\n",
    "sample1.fq_tr_fastqc.html  sample1.fq_tr_fastqc.zip  sample2.fq_tr_fastqc.html  sample2.fq_tr_fastqc.zip\n",
    "\n",
    "multiple_read_out/trimmed_reads:\n",
    "sample1.fq_tr.fq.gz  sample2.fq_tr.fq.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa5dc2",
   "metadata": {},
   "source": [
    "### Step 6: Run a reduced representation bisulfite sequencing (RRBS) Workflow [Using Docker]\n",
    "\n",
    "\n",
    "#### 6a. Find an update-to-containerized version of FastQC\n",
    "![](https://i.imgur.com/hSb5QAW.png)\n",
    "\n",
    "\n",
    "https://hub.docker.com/r/biocontainers/fastqc/tags\n",
    "\n",
    "#### 6a. rrbs_sequencing_docker.nf:\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "\n",
    "process fastqc {\n",
    "  input:\n",
    "    path(read)\n",
    "\n",
    "  publishDir \"${params.out_dir}/fastqc_original_reads\", mode: 'copy'\n",
    "\n",
    "  container 'biocontainers/fastqc:v0.11.9_cv8'\n",
    "  \n",
    "  containerOptions '--bind /gpfs/data/cbc:/gpfs/data/cbc'\n",
    "\n",
    "  output:\n",
    "    path \"*\"\n",
    "\n",
    "  script:\n",
    "    \"\"\"\n",
    "      fastqc ${read}\n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6c. Launch Workflow:\n",
    "```bash\n",
    "nextflow run rrbs_sequencing_docker.nf --read_dir /gpfs/data/cbc/workflow_workshop/input_dir --out_dir docker_out\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
